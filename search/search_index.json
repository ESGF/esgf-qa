{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#esgf-qa","title":"esgf-qa","text":""},{"location":"#quality-assurance-workflow-based-on-compliance-checker-and-cc-plugin-wcrp-or-other-cc-plugins","title":"Quality Assurance Workflow Based on <code>compliance-checker</code> and <code>cc-plugin-wcrp</code> (or other cc-plugins)","text":"<p><code>esgf-qa</code> provides a flexible quality assurance (QA) workflow for evaluating dataset compliance using the ioos/compliance-checker framework (including CF compliance checks) and various community plugins (<code>cc-plugin</code>s), such as ESGF/cc-plugin-wcrp and euro-cordex/cc-plugin-cc6.</p> <p>The tool executes file-based quality control (QC) tests through the Compliance Checker, and, where applicable, performs additional dataset-level checks to test inter-file time-axis continuity and consistency in variable, coordinate and attribute definitions. Results from both file- and dataset-level checks are aggregated, summarized, and clustered for easier interpretation.</p>"},{"location":"#currently-supported-checkers","title":"Currently supported checkers","text":"<p>While <code>esgf-qa</code> has been primarily developed for workflows assessing compliance with WCRP project data specifications (e.g., CMIP, CORDEX), it can also be used for general CF-compliance testing and easily extended to support any <code>cc-plugin</code> and projects following CORDEX- or CMIP-style CMOR table conventions.</p> Standard Checker Name CF Conventions (shipped with ioos/compliance-checker) cf WCRP CMIP6:<ul><li>CMIP6 DRS</li><li>CMIP6 CVs (esgvoc)</li><li>cmip6-cmor-tables (esgvoc)</li></ul> wcrp_cmip6 WCRP CORDEX-CMIP6:<ul><li>CORDEX-CMIP6 Archive Specifications</li><li>cordex-cmip6-cv (esgvoc)</li><li>cordex-cmip6-cmor-tables (esgvoc)</li></ul> wcrp_cordex_cmip6 WCRP CORDEX-CMIP6:<ul><li>CORDEX-CMIP6 Archive Specifications</li><li>cordex-cmip6-cv</li><li>cordex-cmip6-cmor-tables</li></ul> cc6 EERIE:EERIE CMOR Tables &amp; CV eerie Custom MIP (CMOR/MIP tables have to be specified) mip"},{"location":"#installation","title":"Installation","text":""},{"location":"#pip-installation","title":"Pip installation","text":"<pre><code>$ pip install esgf-qa\n</code></pre>"},{"location":"#pip-installation-from-source","title":"Pip installation from source","text":"<p>Clone the repository and <code>cd</code> into the repository folder, then:</p> <pre><code>$ pip install -e .\n</code></pre> <p>Optionally install the dependencies for development:</p> <pre><code>$ pip install -e .[dev]\n</code></pre> <p>See the ioos/compliance-checker for additional Installation notes if problems arise with the dependencies.</p>"},{"location":"#installation-and-setup-of-esgvoc","title":"Installation and setup of <code>esgvoc</code>","text":"<p>The <code>cc-plugin-wcrp</code> checker plugins require the <code>esgvoc</code> software to be installed and setup:</p> <pre><code>pip install esgvoc\nesgvoc config set universe:branch=esgvoc_dev\nesgvoc config add cordex-cmip6\nesgvoc install\n</code></pre> <ul> <li>Test your installation</li> </ul> <p>The following command should now also list the <code>cc-plugin-wcrp</code> checks next to all <code>cc_plugin_cc6</code> and <code>compliance_checker</code> checks:</p> <pre><code>cchecker.py -l\n</code></pre> <p>The following command should now list the necessary projects with metadata sources for <code>esgvoc</code>:</p> <pre><code>esgvoc status\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<pre><code>$ esgqa [-h] [-o &lt;OUTPUT_DIR&gt;] [-t &lt;TEST&gt;] [-O OPTION] [-i &lt;INFO&gt;] [-r] [-C] &lt;parent_dir&gt;\n</code></pre> <ul> <li>positional arguments:</li> <li><code>parent_dir</code>: Parent directory to scan for netCDF-files to check</li> <li>options:</li> <li><code>-h, --help</code>: show this help message and exit</li> <li><code>-o, --output_dir OUTPUT_DIR</code>: Directory to store QA results. Needs to be non-existing or empty or from previous QA run. If not specified, will store results in <code>./cc-qa-check-results/YYYYMMDD-HHmm_&lt;hash&gt;</code>.</li> <li><code>-t, --test TEST</code>: The test to run (<code>'wcrp_cmip6:latest'</code>, <code>'wcrp_cordex_cmip6:latest'</code> or <code>'cf:&lt;version&gt;'</code>, can be specified multiple times, eg.: <code>'-t wcrp_cmip6:latest -t cf:1.7'</code>) - default: running latest CF checks <code>'cf:latest'</code>.</li> <li><code>-O, --option OPTION</code>: Additional options to be passed to the checkers. Format: <code>'&lt;checker&gt;:&lt;option_name&gt;[:&lt;option_value&gt;]'</code>. Multiple invocations possible.</li> <li><code>-i, --info INFO</code>:  Information used to tag the QA results, eg. the simulation id to identify the checked run. Suggested is the original experiment-id you gave the run.</li> <li><code>-r, --resume</code>: Specify to continue a previous QC run. Requires the <code>&lt;output_dir&gt;</code> argument to be set.</li> <li><code>-C, --include_consistency_checks</code>: Include basic consistency and continuity checks. When using the <code>wcrp-*</code>, <code>cc6</code>, <code>mip</code> or <code>eerie</code> checkers, they are included by default.</li> </ul>"},{"location":"#example-usage","title":"Example Usage","text":"<pre><code>$ esgqa -t wcrp_cordex_cmip6:latest -t cf:1.11 -o QA_results/IAEVALL02_2025-10-20 -i \"IAEVALL02\" ESGF_Buff/IAEVALL02/CORDEX-CMIP6\n</code></pre> <p>To resume at a later date, eg. if the QA run did not finish in time or more files have been added to the <code>&lt;parent_dir&gt;</code> (note, that the last modification date of files is NOT taken into account - once a certain file path has been checked it will be marked as checked and checks will only be repeated if runtime errors occured):</p> <pre><code>$ esgqa -o QA_results/IAEVALL02_2025-10-20 -r\n</code></pre> <p>For a custom MIP with defined CMOR tables (<code>\"mip\"</code> is not a placeholder but an actual basic checker of the <code>cc_plugin_cc6</code>):</p> <pre><code>$ esgqa -o /path/to/test/results -t \"mip:latest\" -O \"mip:tables:/path/to/mip_cmor_tables/Tables\" /path/to/MIP/datasets/`\n</code></pre> <p>For CF checks and basic time and consistency / continuity checks:</p> <pre><code>$ esgqa -o /path/to/test/results -t \"cf:1.11\" -C /path/to/datasets/to/check\n</code></pre>"},{"location":"#displaying-the-check-results","title":"Displaying the check results","text":"<p>The results will be stored in two <code>json</code> files: - <code>qa_result_*.json</code>: All failed checks incl. all affected datasets and files are listed. Depending on the number of failed checks and files affected, this file can be quite large in volume (up to GigaBytes). - <code>qa_result_*.cluster.json</code>: The failed checks are clustered and for affected datasets only a single file is referenced as example. This reduces the file size significantly (to usually below 1 MegaByte).</p>"},{"location":"#web-view","title":"Web view","text":"<p>The clustered results can be viewed using the following website:</p> <ul> <li>DKRZ: https://cmiphub.dkrz.de/info/display_qc_results.html.</li> <li>IPSL: coming soon</li> </ul> <p>This website runs entirely in the user's browser using JavaScript, without requiring interaction with a web server. You can select one of the recent QA runs conducted at the respective site or select a local QA run result file to be displayed.</p> <p>Alternatively, you can open the included <code>display_qc_results.html</code> file directly in your browser. While the web view also supports the full (unclustered) results, it is recommended to not use the web view for files greater than a few MegaBytes.</p>"},{"location":"#esgqaviewer","title":"<code>esgqaviewer</code>","text":"<p>The <code>esgqaviewer</code> app can be used to view the result files inside a terminal:</p> <pre><code>esgqaviewer path/to/result.json\n</code></pre> <p>At the bottom of the viewer, all possible tools are listed. The results can be searched using a full text search for instance. A double click with the right mouse button on a node will expand / collapse it and below nodes fully, while a left click will collapse the current node only.</p>"},{"location":"#add-results-to-qa-results-repository","title":"Add results to QA results repository","text":"<ul> <li>DKRZ: https://cmiphub.dkrz.de/info/display_qc_results.html allows viewing QA results hosted in the GitLab Repository qa-results. You can create a Merge Request in that repository to add your own results.</li> <li>IPSL: coming soon</li> <li>Feel free to set up repository for QA results for your institute as well. As example implementation can serve: qa-results</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the Apache License 2.0, and includes the Inter font, which is licensed under the SIL Open Font License 1.1. See the LICENSE file for more details.</p> <p>[!NOTE] This project was originally developed by DKRZ under the name cc-qa (see DKRZ GitLab), with funding from the German Ministry of Research, Technology and Space (BMFTR, reference <code>01LP2326E</code>). It has since been renamed to esgf-qa and is now maintained under the Earth System Grid Federation (ESGF) organization on GitHub.</p> <p>If you previously used <code>cc-qa</code>, please update your installations as described above.</p>"},{"location":"reference/esgf_qa_cluster_results/","title":"Cluster QA results <code>esgf_qa.cluster_results</code>","text":""},{"location":"reference/esgf_qa_cluster_results/#esgf_qa.cluster_results.QAResultAggregator","title":"<code>QAResultAggregator</code>","text":"<p>Aggregate, organize, and cluster the results of multiple ESGF-Quality Assurance (QA) or Climate Checker (cc) runs.</p> <p>This class collects the outcomes of compliance checker (cc) / cc-plugin runs from multiple datasets and files, normalizes them into a consistent internal summary structure, and provides functionality to sort, cluster, and generalize similar messages.</p> <p>Attributes:</p> Name Type Description <code>summary</code> <code>dict of defaultdict</code> <p>Nested dictionary structure that stores the aggregated QA results. It contains two top-level keys:     - <code>\"error\"</code> : maps checker functions to error messages \u2192 dataset IDs \u2192 file names.     - <code>\"fail\"</code>  : maps test weights \u2192 test names \u2192 messages \u2192 dataset IDs \u2192 file names.</p> <code>clustered_summary</code> <code>dict of defaultdict</code> <p>Summary structure produced after clustering messages using :meth:<code>cluster_summary</code>. Keys and nesting mirror <code>summary</code>, but messages are generalized and aggregated/clustered across similar text patterns.</p> <code>checker_dict</code> <code>dict</code> <p>Mapping of checker identifiers to human-readable names, used for consistent labeling in summaries. Only cc checks.</p> <code>checker_dict_ext</code> <code>dict</code> <p>Mapping of checker identifiers to human-readable names, used for consistent labeling in summaries. cc checks extended by esgf_qa checks.</p> <p>Methods:</p> Name Description <code>update</code> <p>Update the summary with a single cc run result (i.e. for one file).</p> <code>update_ds</code> <p>Update the summary with results from a single inter-dataset or inter-file checker run that come with esgf-qa.</p> <code>sort</code> <p>Sort the summary by test weight and test name for consistent output ordering.</p> <code>cluster_messages</code> <p>Cluster similar message strings based on edit-distance similarity.</p> <code>generalize_message_group</code> <p>Derive a generalized message template and placeholder map from a list of similar messages.</p> <code>merge_placeholders</code> <p>Helper to merge adjacent placeholders in message templates where possible.</p> <code>cluster_summary</code> <p>Cluster and generalize all messages in the current summary using a similarity threshold.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from esgf_qa._constants import checker_dict\n&gt;&gt;&gt; agg = QAResultAggregator(checker_dict)\n&gt;&gt;&gt; result = {\n...     \"cf\": {\n...         \"test_1\": {\"value\": (0, 1), \"msgs\": [\"Missing attribute 'units'\"]},\n...     }\n... }\n&gt;&gt;&gt; agg.update(result, dsid=\"dataset_001\", file_name=\"tas_day.nc\")\n&gt;&gt;&gt; agg.sort()\n&gt;&gt;&gt; agg.cluster_summary(threshold=0.8)\n&gt;&gt;&gt; agg.clustered_summary[\"fail\"]\n{3: {'[CF-Conventions] test_1': {'Missing attribute {A} (1 occurrences, e.g. A='units')': {...}}}}\n</code></pre> Source code in <code>esgf_qa/cluster_results.py</code> <pre><code>class QAResultAggregator:\n    \"\"\"\n    Aggregate, organize, and cluster the results of multiple ESGF-Quality Assurance (QA)\n    or Climate Checker (cc) runs.\n\n    This class collects the outcomes of compliance checker (cc) / cc-plugin runs from multiple datasets\n    and files, normalizes them into a consistent internal summary structure, and provides\n    functionality to sort, cluster, and generalize similar messages.\n\n    Attributes\n    ----------\n    summary : dict of defaultdict\n        Nested dictionary structure that stores the aggregated QA results.\n        It contains two top-level keys:\n            - ``\"error\"`` : maps checker functions to error messages \u2192 dataset IDs \u2192 file names.\n            - ``\"fail\"``  : maps test weights \u2192 test names \u2192 messages \u2192 dataset IDs \u2192 file names.\n    clustered_summary : dict of defaultdict\n        Summary structure produced after clustering messages using\n        :meth:`cluster_summary`. Keys and nesting mirror ``summary``, but\n        messages are generalized and aggregated/clustered across similar text patterns.\n    checker_dict : dict\n        Mapping of checker identifiers to human-readable names, used\n        for consistent labeling in summaries. Only cc checks.\n    checker_dict_ext : dict\n        Mapping of checker identifiers to human-readable names, used\n        for consistent labeling in summaries. cc checks extended by esgf_qa checks.\n\n    Methods\n    -------\n    update(result_dict, dsid, file_name)\n        Update the summary with a single cc run result (i.e. for one file).\n    update_ds(result_dict, dsid)\n        Update the summary with results from a single inter-dataset or inter-file checker run\n        that come with esgf-qa.\n    sort()\n        Sort the summary by test weight and test name for consistent output ordering.\n    cluster_messages(messages, threshold)\n        Cluster similar message strings based on edit-distance similarity.\n    generalize_message_group(messages)\n        Derive a generalized message template and placeholder map from a list of similar messages.\n    merge_placeholders(list_of_strings, dictionary, skip=0)\n        Helper to merge adjacent placeholders in message templates where possible.\n    cluster_summary(threshold=0.75)\n        Cluster and generalize all messages in the current summary using a similarity threshold.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from esgf_qa._constants import checker_dict\n    &gt;&gt;&gt; agg = QAResultAggregator(checker_dict)\n    &gt;&gt;&gt; result = {\n    ...     \"cf\": {\n    ...         \"test_1\": {\"value\": (0, 1), \"msgs\": [\"Missing attribute 'units'\"]},\n    ...     }\n    ... }\n    &gt;&gt;&gt; agg.update(result, dsid=\"dataset_001\", file_name=\"tas_day.nc\")\n    &gt;&gt;&gt; agg.sort()\n    &gt;&gt;&gt; agg.cluster_summary(threshold=0.8)\n    &gt;&gt;&gt; agg.clustered_summary[\"fail\"]\n    {3: {'[CF-Conventions] test_1': {'Missing attribute {A} (1 occurrences, e.g. A=\\'units\\')': {...}}}}\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize the aggregator with an empty summary.\n        \"\"\"\n        self.summary = {\n            \"error\": defaultdict(\n                lambda: defaultdict(lambda: defaultdict(list))\n            ),  # No weight, just function -&gt; error msg\n            \"fail\": defaultdict(\n                lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n            ),  # weight -&gt; test -&gt; msg -&gt; dsid -&gt; filenames\n        }\n        self.checker_dict = checker_dict\n        self.checker_dict_ext = checker_dict_ext\n\n    def update(self, result_dict, dsid, file_name):\n        \"\"\"\n        Update the summary with a single result of a cc-run.\n\n        Parameters\n        ----------\n        result_dict : dict\n            Dictionary containing the results of a single cc-run.\n        dsid : str\n            Dataset ID.\n        file_name : str\n            File name.\n        \"\"\"\n        for checker in result_dict:\n            for test in result_dict[checker]:\n                if test == \"errors\":\n                    for function_name, error_msg in result_dict[checker][\n                        \"errors\"\n                    ].items():\n                        self.summary[\"error\"][\n                            f\"[{checker_dict[checker]}] \" + function_name\n                        ][error_msg][dsid].append(file_name)\n                else:\n                    score, max_score = result_dict[checker][test][\"value\"]\n                    weight = result_dict[checker][test].get(\"weight\", 3)\n                    msgs = result_dict[checker][test].get(\"msgs\", [])\n                    if score &lt; max_score:  # test outcome: fail\n                        for msg in msgs:\n                            self.summary[\"fail\"][weight][\n                                f\"[{checker_dict[checker]}] \" + test\n                            ][msg][dsid].append(file_name)\n\n    def update_ds(self, result_dict, dsid):\n        \"\"\"\n        Update the summary with a single result of an esgf-qa (inter-file/dataset) run.\n\n        Parameters\n        ----------\n        result_dict : dict\n            Dictionary containing the results of a single esgf-qa (inter-file/dataset) run.\n        dsid : str\n            Dataset ID.\n        \"\"\"\n        for checker in result_dict:\n            for test in result_dict[checker]:\n                if test == \"errors\":\n                    for function_name, errdict in result_dict[checker][\n                        \"errors\"\n                    ].items():\n                        for file_name in errdict[\"files\"]:\n                            self.summary[\"error\"][\n                                f\"[{checker_dict_ext[checker]}] \" + function_name\n                            ][errdict[\"msg\"]][dsid].append(file_name)\n                else:\n                    weight = result_dict[checker][test].get(\"weight\", 3)\n                    fails = result_dict[checker][test].get(\"msgs\", {})\n                    for msg, file_names in fails.items():\n                        for file_name in file_names:\n                            self.summary[\"fail\"][weight][\n                                f\"[{checker_dict_ext[checker]}] \" + test\n                            ][msg][dsid].append(file_name)\n\n    def sort(self):\n        \"\"\"\n        Sort the summary by test weight and test name for consistent output ordering.\n\n        Modifies the `summary` attribute.\n        \"\"\"\n        self.summary[\"fail\"] = dict(sorted(self.summary[\"fail\"].items(), reverse=True))\n        for key in self.summary[\"fail\"]:\n            self.summary[\"fail\"][key] = dict(sorted(self.summary[\"fail\"][key].items()))\n\n        # Sort errors by function name\n        for checker in self.summary[\"error\"]:\n            self.summary[\"error\"][checker] = dict(\n                sorted(self.summary[\"error\"][checker].items())\n            )\n\n    @staticmethod\n    def cluster_messages(messages, threshold):\n        \"\"\"\n        Cluster messages based on similarity.\n\n        Parameters\n        ----------\n        messages : list\n            List of messages to cluster.\n        threshold : float\n            Similarity threshold.\n\n        Returns\n        -------\n        list\n            List of clusters.\n        \"\"\"\n        clusters = []\n        while messages:\n            base = messages.pop(0)\n            cluster = [base]\n            to_remove = []\n            for msg in messages:\n                ratio = difflib.SequenceMatcher(None, base, msg).ratio()\n                if ratio &gt;= threshold:\n                    cluster.append(msg)\n                    to_remove.append(msg)\n            for msg in to_remove:\n                messages.remove(msg)\n            clusters.append(cluster)\n        return clusters\n\n    @staticmethod\n    def generalize_message_group(messages):\n        \"\"\"\n        Generalize a group of messages.\n\n        Parameters\n        ----------\n        messages : list\n            List of messages to generalize.\n\n        Returns\n        -------\n        str\n            Generalized message.\n        dict\n            Placeholders.\n        \"\"\"\n        if len(messages) == 1:\n            return messages[0], {}\n\n        # Split messages into tokens\n        split_messages = [re.findall(r\"\\w+|\\W\", m) for m in messages]\n        transposed = list(zip(*split_messages))\n        template = []\n        placeholders = {}\n        var_index = 0\n\n        for i, tokens in enumerate(transposed):\n            unique_tokens = set(tokens)\n            if len(unique_tokens) == 1:\n                template.append(tokens[0])\n            else:\n                var_name = chr(ord(\"A\") + var_index)\n                template.append(f\"{{{var_name}}}\")\n                placeholders[var_name] = tokens[0]\n                var_index += 1\n\n        # Merge placeholders if possible\n        template, placeholders = QAResultAggregator.merge_placeholders(\n            template, placeholders\n        )\n\n        # Return the generalized message and the placeholders\n        generalized = \"\".join(template)\n        return generalized, placeholders\n\n    @staticmethod\n    def merge_placeholders(list_of_strings, dictionary, skip=0):\n        \"\"\"\n        Merge adjacent placeholders in message templates where possible.\n\n        Avoids too many placeholders in a clustered message.\n\n        Parameters\n        ----------\n        list_of_strings : list\n            List of strings.\n        dictionary : dict\n            Dictionary of placeholders.\n        skip : int, optional\n            Number of placeholders to skip, by default 0.\n\n        Returns\n        -------\n        list\n            List of strings with placeholders merged.\n        dict\n            Dictionary of placeholders.\n        \"\"\"\n\n        def find_next_two_placeholders(list_of_strings, skip):\n            placeholders = [\n                s for s in list_of_strings if s.startswith(\"{\") and s.endswith(\"}\")\n            ]\n            if len(placeholders) &lt; 2:\n                return None, None\n            return placeholders[skip] if len(placeholders) &gt;= skip + 1 else None, (\n                placeholders[skip + 1] if len(placeholders) &gt;= skip + 2 else None\n            )\n\n        def extract_text_between_placeholders(\n            list_of_strings, placeholder1, placeholder2\n        ):\n            idx1 = list_of_strings.index(placeholder1)\n            idx2 = list_of_strings.index(placeholder2)\n            return \"\".join(list_of_strings[idx1 + 1 : idx2])\n\n        def merge_two_placeholders(\n            placeholder1, placeholder2, text_between, dictionary\n        ):\n            new_value = (\n                dictionary[placeholder1.lstrip(\"{\").rstrip(\"}\")]\n                + text_between\n                + dictionary[placeholder2.lstrip(\"{\").rstrip(\"}\")]\n            )\n            dictionary[placeholder1.lstrip(\"{\").rstrip(\"}\")] = new_value\n            del dictionary[placeholder2.lstrip(\"{\").rstrip(\"}\")]\n            return dictionary\n\n        def update_placeholder_names(list_of_strings, dictionary):\n            old_placeholders = sorted(list(dictionary.keys()))\n            new_placeholders = [\n                chr(ord(\"A\") + i) for i in range(0, len(old_placeholders))\n            ]\n            new_dictionary = dict(\n                zip(new_placeholders, [dictionary[val] for val in old_placeholders])\n            )\n            for old, new in zip(old_placeholders, new_placeholders):\n                list_of_strings = [\n                    s.replace(\"{\" + old + \"}\", \"{\" + new + \"}\") for s in list_of_strings\n                ]\n            return list_of_strings, new_dictionary\n\n        def replace_placeholders_with_new_one(\n            list_of_strings, placeholder1, placeholder2\n        ):\n            idx1 = list_of_strings.index(placeholder1)\n            idx2 = list_of_strings.index(placeholder2)\n            list_of_strings_new = list_of_strings[:idx1] + [placeholder1]\n            if idx2 &lt; len(list_of_strings) + 1:\n                list_of_strings_new += list_of_strings[idx2 + 1 :]\n            return list_of_strings_new\n\n        if not any(s.startswith(\"{\") and s.endswith(\"}\") for s in list_of_strings):\n            return list_of_strings, dictionary\n\n        placeholder1, placeholder2 = find_next_two_placeholders(list_of_strings, skip)\n        if placeholder1 is None or placeholder2 is None:\n            return list_of_strings, dictionary\n\n        text_between = extract_text_between_placeholders(\n            list_of_strings, placeholder1, placeholder2\n        )\n        if len(text_between) &lt; 5:\n            dictionary = merge_two_placeholders(\n                placeholder1, placeholder2, text_between, dictionary\n            )\n            list_of_strings = replace_placeholders_with_new_one(\n                list_of_strings, placeholder1, placeholder2\n            )\n            list_of_strings, dictionary = update_placeholder_names(\n                list_of_strings, dictionary\n            )\n            return QAResultAggregator.merge_placeholders(\n                list_of_strings, dictionary, skip\n            )\n        else:\n            return QAResultAggregator.merge_placeholders(\n                list_of_strings, dictionary, skip + 1\n            )\n\n    def cluster_summary(self, threshold=0.75):\n        \"\"\"\n        Cluster messages in the summary into groups of similar messages.\n\n        Drastically reduces number of messages in the summary for datasets accumulating\n        large numbers of check failure messages.\n\n        Parameters\n        ----------\n        threshold : float, optional\n            The threshold for similarity between messages, by default 0.75.\n\n        Returns\n        -------\n        None\n            Modifies the `clustered_summary` attribute.\n        \"\"\"\n        self.clustered_summary = defaultdict(\n            lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))\n        )\n        for status in self.summary:\n            if status == \"error\":\n                for test_id in self.summary[status]:\n                    messages = list(self.summary[status][test_id].keys())\n                    # Pass a copy of messages to cluster_messages to generate clusters\n                    clusters = QAResultAggregator.cluster_messages(\n                        messages[:], threshold\n                    )\n\n                    for cluster in clusters:\n                        generalized, placeholders = (\n                            QAResultAggregator.generalize_message_group(cluster)\n                        )\n                        example_parts = \", \".join(\n                            [\n                                (\n                                    f\"{k}='{v[0]}'\"\n                                    if isinstance(v, list)\n                                    else f\"{k}='{v}'\"\n                                )\n                                for k, v in placeholders.items()\n                            ]\n                        )\n                        if example_parts:\n                            msg_summary = f\"{generalized} ({len(cluster)} occurrences, e.g. {example_parts})\"\n                        else:\n                            msg_summary = f\"{generalized}{' (' + str(len(cluster)) + ' occurrences)' if len(cluster) &gt; 1 else ''}\"\n\n                        # Gather all ds_ids and filenames across the cluster\n                        combined = defaultdict(set)\n                        for message in cluster:\n                            for ds_id, files in self.summary[status][test_id][\n                                message\n                            ].items():\n                                combined[ds_id].update(files)\n\n                        # Shorten file lists to one example\n                        formatted = {\n                            ds_id\n                            + \" (\"\n                            + str(len(files))\n                            + f\" file{'s' if len(files) &gt; 1 else ''} affected)\": (\n                                [f\"e.g. '{next(iter(files))}'\"]\n                                if len(files) &gt; 1\n                                else [f\"'{next(iter(files))}'\"]\n                            )\n                            for ds_id, files in combined.items()\n                        }\n\n                        self.clustered_summary[status][test_id][msg_summary] = formatted\n            elif status == \"fail\":\n                for weight in self.summary[status]:\n                    for test_id in self.summary[status][weight]:\n                        messages = list(self.summary[status][weight][test_id].keys())\n                        # Pass a copy of messages to cluster_messages to generate clusters\n                        clusters = QAResultAggregator.cluster_messages(\n                            messages[:], threshold\n                        )\n\n                        for cluster in clusters:\n                            generalized, placeholders = (\n                                QAResultAggregator.generalize_message_group(cluster)\n                            )\n                            example_parts = \", \".join(\n                                [\n                                    (\n                                        f\"{k}='{v[0]}'\"\n                                        if isinstance(v, list)\n                                        else f\"{k}='{v}'\"\n                                    )\n                                    for k, v in placeholders.items()\n                                ]\n                            )\n                            if example_parts:\n                                msg_summary = f\"{generalized} ({len(cluster)} occurrences, e.g. {example_parts})\"\n                            else:\n                                msg_summary = f\"{generalized}{' (' + str(len(cluster)) + ' occurrences)' if len(cluster) &gt; 1 else ''}\"\n\n                            # Gather all ds_ids and filenames across the cluster\n                            combined = defaultdict(set)\n                            for message in cluster:\n                                for ds_id, files in self.summary[status][weight][\n                                    test_id\n                                ][message].items():\n                                    combined[ds_id].update(files)\n\n                            # Shorten file lists to one example\n                            formatted = {\n                                ds_id\n                                + \" (\"\n                                + str(len(files))\n                                + f\" file{'s' if len(files) &gt; 1 else ''} affected)\": (\n                                    [f\"e.g. '{next(iter(files))}'\"]\n                                    if len(files) &gt; 1\n                                    else [f\"'{next(iter(files))}'\"]\n                                )\n                                for ds_id, files in combined.items()\n                            }\n\n                            self.clustered_summary[status][weight][test_id][\n                                msg_summary\n                            ] = formatted\n</code></pre>"},{"location":"reference/esgf_qa_cluster_results/#esgf_qa.cluster_results.QAResultAggregator.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the aggregator with an empty summary.</p> Source code in <code>esgf_qa/cluster_results.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initialize the aggregator with an empty summary.\n    \"\"\"\n    self.summary = {\n        \"error\": defaultdict(\n            lambda: defaultdict(lambda: defaultdict(list))\n        ),  # No weight, just function -&gt; error msg\n        \"fail\": defaultdict(\n            lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n        ),  # weight -&gt; test -&gt; msg -&gt; dsid -&gt; filenames\n    }\n    self.checker_dict = checker_dict\n    self.checker_dict_ext = checker_dict_ext\n</code></pre>"},{"location":"reference/esgf_qa_cluster_results/#esgf_qa.cluster_results.QAResultAggregator.cluster_messages","title":"<code>cluster_messages(messages, threshold)</code>  <code>staticmethod</code>","text":"<p>Cluster messages based on similarity.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list</code> <p>List of messages to cluster.</p> required <code>threshold</code> <code>float</code> <p>Similarity threshold.</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of clusters.</p> Source code in <code>esgf_qa/cluster_results.py</code> <pre><code>@staticmethod\ndef cluster_messages(messages, threshold):\n    \"\"\"\n    Cluster messages based on similarity.\n\n    Parameters\n    ----------\n    messages : list\n        List of messages to cluster.\n    threshold : float\n        Similarity threshold.\n\n    Returns\n    -------\n    list\n        List of clusters.\n    \"\"\"\n    clusters = []\n    while messages:\n        base = messages.pop(0)\n        cluster = [base]\n        to_remove = []\n        for msg in messages:\n            ratio = difflib.SequenceMatcher(None, base, msg).ratio()\n            if ratio &gt;= threshold:\n                cluster.append(msg)\n                to_remove.append(msg)\n        for msg in to_remove:\n            messages.remove(msg)\n        clusters.append(cluster)\n    return clusters\n</code></pre>"},{"location":"reference/esgf_qa_cluster_results/#esgf_qa.cluster_results.QAResultAggregator.cluster_summary","title":"<code>cluster_summary(threshold=0.75)</code>","text":"<p>Cluster messages in the summary into groups of similar messages.</p> <p>Drastically reduces number of messages in the summary for datasets accumulating large numbers of check failure messages.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>The threshold for similarity between messages, by default 0.75.</p> <code>0.75</code> <p>Returns:</p> Type Description <code>None</code> <p>Modifies the <code>clustered_summary</code> attribute.</p> Source code in <code>esgf_qa/cluster_results.py</code> <pre><code>def cluster_summary(self, threshold=0.75):\n    \"\"\"\n    Cluster messages in the summary into groups of similar messages.\n\n    Drastically reduces number of messages in the summary for datasets accumulating\n    large numbers of check failure messages.\n\n    Parameters\n    ----------\n    threshold : float, optional\n        The threshold for similarity between messages, by default 0.75.\n\n    Returns\n    -------\n    None\n        Modifies the `clustered_summary` attribute.\n    \"\"\"\n    self.clustered_summary = defaultdict(\n        lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))\n    )\n    for status in self.summary:\n        if status == \"error\":\n            for test_id in self.summary[status]:\n                messages = list(self.summary[status][test_id].keys())\n                # Pass a copy of messages to cluster_messages to generate clusters\n                clusters = QAResultAggregator.cluster_messages(\n                    messages[:], threshold\n                )\n\n                for cluster in clusters:\n                    generalized, placeholders = (\n                        QAResultAggregator.generalize_message_group(cluster)\n                    )\n                    example_parts = \", \".join(\n                        [\n                            (\n                                f\"{k}='{v[0]}'\"\n                                if isinstance(v, list)\n                                else f\"{k}='{v}'\"\n                            )\n                            for k, v in placeholders.items()\n                        ]\n                    )\n                    if example_parts:\n                        msg_summary = f\"{generalized} ({len(cluster)} occurrences, e.g. {example_parts})\"\n                    else:\n                        msg_summary = f\"{generalized}{' (' + str(len(cluster)) + ' occurrences)' if len(cluster) &gt; 1 else ''}\"\n\n                    # Gather all ds_ids and filenames across the cluster\n                    combined = defaultdict(set)\n                    for message in cluster:\n                        for ds_id, files in self.summary[status][test_id][\n                            message\n                        ].items():\n                            combined[ds_id].update(files)\n\n                    # Shorten file lists to one example\n                    formatted = {\n                        ds_id\n                        + \" (\"\n                        + str(len(files))\n                        + f\" file{'s' if len(files) &gt; 1 else ''} affected)\": (\n                            [f\"e.g. '{next(iter(files))}'\"]\n                            if len(files) &gt; 1\n                            else [f\"'{next(iter(files))}'\"]\n                        )\n                        for ds_id, files in combined.items()\n                    }\n\n                    self.clustered_summary[status][test_id][msg_summary] = formatted\n        elif status == \"fail\":\n            for weight in self.summary[status]:\n                for test_id in self.summary[status][weight]:\n                    messages = list(self.summary[status][weight][test_id].keys())\n                    # Pass a copy of messages to cluster_messages to generate clusters\n                    clusters = QAResultAggregator.cluster_messages(\n                        messages[:], threshold\n                    )\n\n                    for cluster in clusters:\n                        generalized, placeholders = (\n                            QAResultAggregator.generalize_message_group(cluster)\n                        )\n                        example_parts = \", \".join(\n                            [\n                                (\n                                    f\"{k}='{v[0]}'\"\n                                    if isinstance(v, list)\n                                    else f\"{k}='{v}'\"\n                                )\n                                for k, v in placeholders.items()\n                            ]\n                        )\n                        if example_parts:\n                            msg_summary = f\"{generalized} ({len(cluster)} occurrences, e.g. {example_parts})\"\n                        else:\n                            msg_summary = f\"{generalized}{' (' + str(len(cluster)) + ' occurrences)' if len(cluster) &gt; 1 else ''}\"\n\n                        # Gather all ds_ids and filenames across the cluster\n                        combined = defaultdict(set)\n                        for message in cluster:\n                            for ds_id, files in self.summary[status][weight][\n                                test_id\n                            ][message].items():\n                                combined[ds_id].update(files)\n\n                        # Shorten file lists to one example\n                        formatted = {\n                            ds_id\n                            + \" (\"\n                            + str(len(files))\n                            + f\" file{'s' if len(files) &gt; 1 else ''} affected)\": (\n                                [f\"e.g. '{next(iter(files))}'\"]\n                                if len(files) &gt; 1\n                                else [f\"'{next(iter(files))}'\"]\n                            )\n                            for ds_id, files in combined.items()\n                        }\n\n                        self.clustered_summary[status][weight][test_id][\n                            msg_summary\n                        ] = formatted\n</code></pre>"},{"location":"reference/esgf_qa_cluster_results/#esgf_qa.cluster_results.QAResultAggregator.generalize_message_group","title":"<code>generalize_message_group(messages)</code>  <code>staticmethod</code>","text":"<p>Generalize a group of messages.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list</code> <p>List of messages to generalize.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Generalized message.</p> <code>dict</code> <p>Placeholders.</p> Source code in <code>esgf_qa/cluster_results.py</code> <pre><code>@staticmethod\ndef generalize_message_group(messages):\n    \"\"\"\n    Generalize a group of messages.\n\n    Parameters\n    ----------\n    messages : list\n        List of messages to generalize.\n\n    Returns\n    -------\n    str\n        Generalized message.\n    dict\n        Placeholders.\n    \"\"\"\n    if len(messages) == 1:\n        return messages[0], {}\n\n    # Split messages into tokens\n    split_messages = [re.findall(r\"\\w+|\\W\", m) for m in messages]\n    transposed = list(zip(*split_messages))\n    template = []\n    placeholders = {}\n    var_index = 0\n\n    for i, tokens in enumerate(transposed):\n        unique_tokens = set(tokens)\n        if len(unique_tokens) == 1:\n            template.append(tokens[0])\n        else:\n            var_name = chr(ord(\"A\") + var_index)\n            template.append(f\"{{{var_name}}}\")\n            placeholders[var_name] = tokens[0]\n            var_index += 1\n\n    # Merge placeholders if possible\n    template, placeholders = QAResultAggregator.merge_placeholders(\n        template, placeholders\n    )\n\n    # Return the generalized message and the placeholders\n    generalized = \"\".join(template)\n    return generalized, placeholders\n</code></pre>"},{"location":"reference/esgf_qa_cluster_results/#esgf_qa.cluster_results.QAResultAggregator.merge_placeholders","title":"<code>merge_placeholders(list_of_strings, dictionary, skip=0)</code>  <code>staticmethod</code>","text":"<p>Merge adjacent placeholders in message templates where possible.</p> <p>Avoids too many placeholders in a clustered message.</p> <p>Parameters:</p> Name Type Description Default <code>list_of_strings</code> <code>list</code> <p>List of strings.</p> required <code>dictionary</code> <code>dict</code> <p>Dictionary of placeholders.</p> required <code>skip</code> <code>int</code> <p>Number of placeholders to skip, by default 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>list</code> <p>List of strings with placeholders merged.</p> <code>dict</code> <p>Dictionary of placeholders.</p> Source code in <code>esgf_qa/cluster_results.py</code> <pre><code>@staticmethod\ndef merge_placeholders(list_of_strings, dictionary, skip=0):\n    \"\"\"\n    Merge adjacent placeholders in message templates where possible.\n\n    Avoids too many placeholders in a clustered message.\n\n    Parameters\n    ----------\n    list_of_strings : list\n        List of strings.\n    dictionary : dict\n        Dictionary of placeholders.\n    skip : int, optional\n        Number of placeholders to skip, by default 0.\n\n    Returns\n    -------\n    list\n        List of strings with placeholders merged.\n    dict\n        Dictionary of placeholders.\n    \"\"\"\n\n    def find_next_two_placeholders(list_of_strings, skip):\n        placeholders = [\n            s for s in list_of_strings if s.startswith(\"{\") and s.endswith(\"}\")\n        ]\n        if len(placeholders) &lt; 2:\n            return None, None\n        return placeholders[skip] if len(placeholders) &gt;= skip + 1 else None, (\n            placeholders[skip + 1] if len(placeholders) &gt;= skip + 2 else None\n        )\n\n    def extract_text_between_placeholders(\n        list_of_strings, placeholder1, placeholder2\n    ):\n        idx1 = list_of_strings.index(placeholder1)\n        idx2 = list_of_strings.index(placeholder2)\n        return \"\".join(list_of_strings[idx1 + 1 : idx2])\n\n    def merge_two_placeholders(\n        placeholder1, placeholder2, text_between, dictionary\n    ):\n        new_value = (\n            dictionary[placeholder1.lstrip(\"{\").rstrip(\"}\")]\n            + text_between\n            + dictionary[placeholder2.lstrip(\"{\").rstrip(\"}\")]\n        )\n        dictionary[placeholder1.lstrip(\"{\").rstrip(\"}\")] = new_value\n        del dictionary[placeholder2.lstrip(\"{\").rstrip(\"}\")]\n        return dictionary\n\n    def update_placeholder_names(list_of_strings, dictionary):\n        old_placeholders = sorted(list(dictionary.keys()))\n        new_placeholders = [\n            chr(ord(\"A\") + i) for i in range(0, len(old_placeholders))\n        ]\n        new_dictionary = dict(\n            zip(new_placeholders, [dictionary[val] for val in old_placeholders])\n        )\n        for old, new in zip(old_placeholders, new_placeholders):\n            list_of_strings = [\n                s.replace(\"{\" + old + \"}\", \"{\" + new + \"}\") for s in list_of_strings\n            ]\n        return list_of_strings, new_dictionary\n\n    def replace_placeholders_with_new_one(\n        list_of_strings, placeholder1, placeholder2\n    ):\n        idx1 = list_of_strings.index(placeholder1)\n        idx2 = list_of_strings.index(placeholder2)\n        list_of_strings_new = list_of_strings[:idx1] + [placeholder1]\n        if idx2 &lt; len(list_of_strings) + 1:\n            list_of_strings_new += list_of_strings[idx2 + 1 :]\n        return list_of_strings_new\n\n    if not any(s.startswith(\"{\") and s.endswith(\"}\") for s in list_of_strings):\n        return list_of_strings, dictionary\n\n    placeholder1, placeholder2 = find_next_two_placeholders(list_of_strings, skip)\n    if placeholder1 is None or placeholder2 is None:\n        return list_of_strings, dictionary\n\n    text_between = extract_text_between_placeholders(\n        list_of_strings, placeholder1, placeholder2\n    )\n    if len(text_between) &lt; 5:\n        dictionary = merge_two_placeholders(\n            placeholder1, placeholder2, text_between, dictionary\n        )\n        list_of_strings = replace_placeholders_with_new_one(\n            list_of_strings, placeholder1, placeholder2\n        )\n        list_of_strings, dictionary = update_placeholder_names(\n            list_of_strings, dictionary\n        )\n        return QAResultAggregator.merge_placeholders(\n            list_of_strings, dictionary, skip\n        )\n    else:\n        return QAResultAggregator.merge_placeholders(\n            list_of_strings, dictionary, skip + 1\n        )\n</code></pre>"},{"location":"reference/esgf_qa_cluster_results/#esgf_qa.cluster_results.QAResultAggregator.sort","title":"<code>sort()</code>","text":"<p>Sort the summary by test weight and test name for consistent output ordering.</p> <p>Modifies the <code>summary</code> attribute.</p> Source code in <code>esgf_qa/cluster_results.py</code> <pre><code>def sort(self):\n    \"\"\"\n    Sort the summary by test weight and test name for consistent output ordering.\n\n    Modifies the `summary` attribute.\n    \"\"\"\n    self.summary[\"fail\"] = dict(sorted(self.summary[\"fail\"].items(), reverse=True))\n    for key in self.summary[\"fail\"]:\n        self.summary[\"fail\"][key] = dict(sorted(self.summary[\"fail\"][key].items()))\n\n    # Sort errors by function name\n    for checker in self.summary[\"error\"]:\n        self.summary[\"error\"][checker] = dict(\n            sorted(self.summary[\"error\"][checker].items())\n        )\n</code></pre>"},{"location":"reference/esgf_qa_cluster_results/#esgf_qa.cluster_results.QAResultAggregator.update","title":"<code>update(result_dict, dsid, file_name)</code>","text":"<p>Update the summary with a single result of a cc-run.</p> <p>Parameters:</p> Name Type Description Default <code>result_dict</code> <code>dict</code> <p>Dictionary containing the results of a single cc-run.</p> required <code>dsid</code> <code>str</code> <p>Dataset ID.</p> required <code>file_name</code> <code>str</code> <p>File name.</p> required Source code in <code>esgf_qa/cluster_results.py</code> <pre><code>def update(self, result_dict, dsid, file_name):\n    \"\"\"\n    Update the summary with a single result of a cc-run.\n\n    Parameters\n    ----------\n    result_dict : dict\n        Dictionary containing the results of a single cc-run.\n    dsid : str\n        Dataset ID.\n    file_name : str\n        File name.\n    \"\"\"\n    for checker in result_dict:\n        for test in result_dict[checker]:\n            if test == \"errors\":\n                for function_name, error_msg in result_dict[checker][\n                    \"errors\"\n                ].items():\n                    self.summary[\"error\"][\n                        f\"[{checker_dict[checker]}] \" + function_name\n                    ][error_msg][dsid].append(file_name)\n            else:\n                score, max_score = result_dict[checker][test][\"value\"]\n                weight = result_dict[checker][test].get(\"weight\", 3)\n                msgs = result_dict[checker][test].get(\"msgs\", [])\n                if score &lt; max_score:  # test outcome: fail\n                    for msg in msgs:\n                        self.summary[\"fail\"][weight][\n                            f\"[{checker_dict[checker]}] \" + test\n                        ][msg][dsid].append(file_name)\n</code></pre>"},{"location":"reference/esgf_qa_cluster_results/#esgf_qa.cluster_results.QAResultAggregator.update_ds","title":"<code>update_ds(result_dict, dsid)</code>","text":"<p>Update the summary with a single result of an esgf-qa (inter-file/dataset) run.</p> <p>Parameters:</p> Name Type Description Default <code>result_dict</code> <code>dict</code> <p>Dictionary containing the results of a single esgf-qa (inter-file/dataset) run.</p> required <code>dsid</code> <code>str</code> <p>Dataset ID.</p> required Source code in <code>esgf_qa/cluster_results.py</code> <pre><code>def update_ds(self, result_dict, dsid):\n    \"\"\"\n    Update the summary with a single result of an esgf-qa (inter-file/dataset) run.\n\n    Parameters\n    ----------\n    result_dict : dict\n        Dictionary containing the results of a single esgf-qa (inter-file/dataset) run.\n    dsid : str\n        Dataset ID.\n    \"\"\"\n    for checker in result_dict:\n        for test in result_dict[checker]:\n            if test == \"errors\":\n                for function_name, errdict in result_dict[checker][\n                    \"errors\"\n                ].items():\n                    for file_name in errdict[\"files\"]:\n                        self.summary[\"error\"][\n                            f\"[{checker_dict_ext[checker]}] \" + function_name\n                        ][errdict[\"msg\"]][dsid].append(file_name)\n            else:\n                weight = result_dict[checker][test].get(\"weight\", 3)\n                fails = result_dict[checker][test].get(\"msgs\", {})\n                for msg, file_names in fails.items():\n                    for file_name in file_names:\n                        self.summary[\"fail\"][weight][\n                            f\"[{checker_dict_ext[checker]}] \" + test\n                        ][msg][dsid].append(file_name)\n</code></pre>"},{"location":"reference/esgf_qa_con_checks/","title":"Consistency Checks <code>esgf_qa.con_checks</code>","text":""},{"location":"reference/esgf_qa_con_checks/#esgf_qa.con_checks.compare_dicts","title":"<code>compare_dicts(dict1, dict2, exclude_keys=None)</code>","text":"<p>Compare two dictionaries and return keys with differing values.</p> <p>Parameters:</p> Name Type Description Default <code>dict1</code> <code>dict</code> <p>First dictionary to compare.</p> required <code>dict2</code> <code>dict</code> <p>Second dictionary to compare.</p> required <code>exclude_keys</code> <code>list</code> <p>List of keys to exclude from comparison.</p> <code>None</code> <p>Returns:</p> Type Description <code>list</code> <p>List of keys with differing values.</p> Source code in <code>esgf_qa/con_checks.py</code> <pre><code>def compare_dicts(dict1, dict2, exclude_keys=None):\n    \"\"\"\n    Compare two dictionaries and return keys with differing values.\n\n    Parameters\n    ----------\n    dict1 : dict\n        First dictionary to compare.\n    dict2 : dict\n        Second dictionary to compare.\n    exclude_keys : list, optional\n        List of keys to exclude from comparison.\n\n    Returns\n    -------\n    list\n        List of keys with differing values.\n    \"\"\"\n    if exclude_keys is None:\n        exclude_keys = set()\n    else:\n        exclude_keys = set(exclude_keys)\n\n    # Get all keys that are in either dictionary, excluding the ones to skip\n    all_keys = (set(dict1) | set(dict2)) - exclude_keys\n\n    # Collect keys with differing values\n    differing_keys = [key for key in all_keys if dict1.get(key) != dict2.get(key)]\n\n    return differing_keys\n</code></pre>"},{"location":"reference/esgf_qa_con_checks/#esgf_qa.con_checks.compare_nested_dicts","title":"<code>compare_nested_dicts(dict1, dict2, exclude_keys=None)</code>","text":"<p>Compare two nested dictionaries and return keys with differing values.</p> <p>Parameters:</p> Name Type Description Default <code>dict1</code> <code>dict</code> <p>First dictionary to compare.</p> required <code>dict2</code> <code>dict</code> <p>Second dictionary to compare.</p> required <code>exclude_keys</code> <code>list</code> <p>List of keys to exclude from comparison.</p> <code>None</code> <p>Returns:</p> Type Description <code>list</code> <p>List of keys with differing values.</p> Source code in <code>esgf_qa/con_checks.py</code> <pre><code>def compare_nested_dicts(dict1, dict2, exclude_keys=None):\n    \"\"\"\n    Compare two nested dictionaries and return keys with differing values.\n\n    Parameters\n    ----------\n    dict1 : dict\n        First dictionary to compare.\n    dict2 : dict\n        Second dictionary to compare.\n    exclude_keys : list, optional\n        List of keys to exclude from comparison.\n\n    Returns\n    -------\n    list\n        List of keys with differing values.\n    \"\"\"\n    diffs = {}\n\n    all_root_keys = set(dict1) | set(dict2)\n\n    for root_key in all_root_keys:\n        subdict1 = dict1.get(root_key, {})\n        subdict2 = dict2.get(root_key, {})\n\n        if not isinstance(subdict1, dict) or not isinstance(subdict2, dict):\n            if subdict1 != subdict2:\n                diffs[root_key] = []\n            continue\n\n        diffs_k = compare_dicts(subdict1, subdict2, exclude_keys)\n\n        if diffs_k:\n            diffs[root_key] = diffs_k\n\n    return diffs\n</code></pre>"},{"location":"reference/esgf_qa_con_checks/#esgf_qa.con_checks.compatibility_checks","title":"<code>compatibility_checks(ds, ds_map, files_to_check_dict, checker_options)</code>","text":"<p>Compatibility checks for a dataset.</p> <p>Checks for:</p> <pre><code>- xarray open_mfdataset (compat='override', join='outer')\n- xarray open_mfdataset (compat='no_conflicts', join='exact')\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>str</code> <p>Dataset to process.</p> required <code>ds_map</code> <code>dict</code> <p>Dictionary mapping dataset IDs to file paths.</p> required <code>files_to_check_dict</code> <code>dict</code> <p>A special dictionary mapping files to check to datasets.</p> required <code>checker_options</code> <code>dict</code> <p>Dictionary of checker options.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of results.</p> Source code in <code>esgf_qa/con_checks.py</code> <pre><code>def compatibility_checks(ds, ds_map, files_to_check_dict, checker_options):\n    \"\"\"\n    Compatibility checks for a dataset.\n\n    Checks for:\n\n        - xarray open_mfdataset (compat='override', join='outer')\n        - xarray open_mfdataset (compat='no_conflicts', join='exact')\n\n    Parameters\n    ----------\n    ds : str\n        Dataset to process.\n    ds_map : dict\n        Dictionary mapping dataset IDs to file paths.\n    files_to_check_dict : dict\n        A special dictionary mapping files to check to datasets.\n    checker_options : dict\n        Dictionary of checker options.\n\n    Returns\n    -------\n    dict\n        Dictionary of results.\n    \"\"\"\n    results = defaultdict(level1_factory)\n    filelist = sorted(ds_map[ds])\n\n    # open_mfdataset - override\n    test = \"xarray open_mfdataset (compat='override', join='outer')\"\n    results[test][\"weight\"] = 3\n    try:\n        with xr.open_mfdataset(\n            filelist, coords=\"minimal\", compat=\"override\", data_vars=\"all\", join=\"outer\"\n        ) as ds:\n            pass\n    except Exception as e:\n        results[test][\"msgs\"][str(e)].extend(filelist)\n\n    # open_mfdataset - no_conflicts\n    test = \"xarray open_mfdataset (compat='no_conflicts', join='exact')\"\n    results[test][\"weight\"] = 3\n    try:\n        with xr.open_mfdataset(\n            filelist,\n            coords=\"minimal\",\n            compat=\"no_conflicts\",\n            data_vars=\"all\",\n            join=\"exact\",\n        ) as ds:\n            pass\n    except Exception as e:\n        results[test][\"msgs\"][str(e)].extend(filelist)\n\n    return results\n</code></pre>"},{"location":"reference/esgf_qa_con_checks/#esgf_qa.con_checks.consistency_checks","title":"<code>consistency_checks(ds, ds_map, files_to_check_dict, checker_options)</code>","text":"<p>Consistency checks.</p> <p>Runs inter-file consistency checks on a dataset:</p> <pre><code>- Global attributes (values and data types)\n- Variable attributes (values and data types)\n- Coordinates (values)\n- Dimensions (names and sizes)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>str</code> <p>Dataset to process.</p> required <code>ds_map</code> <code>dict</code> <p>Dictionary mapping dataset IDs to file paths.</p> required <code>files_to_check_dict</code> <code>dict</code> <p>A special dictionary mapping files to check to datasets.</p> required <code>checker_options</code> <code>dict</code> <p>Dictionary of checker options.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the results of the consistency checks.</p> Source code in <code>esgf_qa/con_checks.py</code> <pre><code>def consistency_checks(ds, ds_map, files_to_check_dict, checker_options):\n    \"\"\"\n    Consistency checks.\n\n    Runs inter-file consistency checks on a dataset:\n\n        - Global attributes (values and data types)\n        - Variable attributes (values and data types)\n        - Coordinates (values)\n        - Dimensions (names and sizes)\n\n    Parameters\n    ----------\n    ds : str\n        Dataset to process.\n    ds_map : dict\n        Dictionary mapping dataset IDs to file paths.\n    files_to_check_dict : dict\n        A special dictionary mapping files to check to datasets.\n    checker_options : dict\n        Dictionary of checker options.\n\n    Returns\n    -------\n    dict\n        A dictionary containing the results of the consistency checks.\n    \"\"\"\n    results = defaultdict(level1_factory)\n    filelist = sorted(ds_map[ds])\n    consistency_files = OrderedDict(\n        (files_to_check_dict[i][\"consistency_file\"], i) for i in filelist\n    )\n\n    # Exclude the following global attributes from comparison\n    excl_global_attrs = [\"creation_date\", \"history\", \"tracking_id\"]\n\n    # Exclude the following variable attributes from comparison\n    excl_var_attrs = []\n\n    # Exclude the following coordinates from comparison\n    excl_coords = []\n\n    # Compare each file with reference\n    reference_file = list(consistency_files.keys())[0]\n    with open(reference_file) as fr:\n        reference_data = json.load(fr)\n        for file in consistency_files.keys():\n            if file == reference_file:\n                continue\n            with open(file) as fc:\n                data = json.load(fc)\n\n                # Compare required global attributes\n                test = \"Required global attributes\"\n                results[test][\"weight\"] = 3\n                diff_keys = compare_dicts(\n                    reference_data[\"global_attributes\"],\n                    data[\"global_attributes\"],\n                    exclude_keys=excl_global_attrs,\n                )\n                if diff_keys:\n                    err_msg = \"The following global attributes differ: \" + \", \".join(\n                        sorted(diff_keys)\n                    )\n                    results[test][\"msgs\"][err_msg].append(consistency_files[file])\n\n                # Compare non-required global attributes\n                test = \"Non-required global attributes\"\n                results[test][\"weight\"] = 1\n                diff_keys = compare_dicts(\n                    reference_data[\"global_attributes_non_required\"],\n                    data[\"global_attributes_non_required\"],\n                    exclude_keys=excl_global_attrs,\n                )\n                if diff_keys:\n                    err_msg = (\n                        \"The following non-required global attributes differ: \"\n                        + \", \".join(sorted(diff_keys))\n                    )\n                    results[test][\"msgs\"][err_msg].append(consistency_files[file])\n\n                # Compare global attributes dtypes\n                test = \"Global attributes data types\"\n                results[test][\"weight\"] = 3\n                diff_keys = compare_dicts(\n                    reference_data[\"global_attributes_dtypes\"],\n                    data[\"global_attributes_dtypes\"],\n                    exclude_keys=[],\n                )\n                if diff_keys:\n                    diff_keys = [\n                        key\n                        for key in diff_keys\n                        if key in reference_data[\"global_attributes_dtypes\"]\n                        and key in data[\"global_attributes_dtypes\"]\n                    ]\n                    if diff_keys:\n                        err_msg = (\n                            \"The following global attributes have inconsistent data types: \"\n                            + \", \".join(sorted(diff_keys))\n                        )\n                        results[test][\"msgs\"][err_msg].append(consistency_files[file])\n\n                # Compare variable attributes\n                test = \"Variable attributes\"\n                results[test][\"weight\"] = 3\n                diff_keys = compare_nested_dicts(\n                    reference_data[\"variable_attributes\"],\n                    data[\"variable_attributes\"],\n                    exclude_keys=excl_var_attrs,\n                )\n                if diff_keys:\n                    for key, diff in diff_keys.items():\n                        if diff:\n                            err_msg = (\n                                f\"For variable '{key}' the following variable attributes differ: \"\n                                + \", \".join(sorted(diff))\n                            )\n                            results[test][\"msgs\"][err_msg].append(\n                                consistency_files[file]\n                            )\n                        else:\n                            err_msg = f\"Variable '{key}' not present.\"\n                            if key not in data[\"variable_attributes\"]:\n                                results[test][\"msgs\"][err_msg].append(\n                                    consistency_files[file]\n                                )\n                            else:\n                                results[test][\"msgs\"][err_msg].append(\n                                    consistency_files[reference_file]\n                                )\n\n                # Compare variable attributes data types\n                test = \"Variable attributes data types\"\n                results[test][\"weight\"] = 3\n                diff_keys = compare_nested_dicts(\n                    reference_data[\"variable_attributes_dtypes\"],\n                    data[\"variable_attributes_dtypes\"],\n                    exclude_keys=[],\n                )\n                if diff_keys:\n                    for key, diff in diff_keys.items():\n                        if diff:\n                            err_msg = (\n                                f\"For variable '{key}' the following variable attributes have inconsistent data types: \"\n                                + \", \".join(sorted(diff))\n                            )\n                            results[test][\"msgs\"][err_msg].append(\n                                consistency_files[file]\n                            )\n\n                # Compare dimensions\n                test = \"Dimensions\"\n                results[test][\"weight\"] = 3\n                diff_keys = compare_dicts(\n                    reference_data[\"dimensions\"],\n                    data[\"dimensions\"],\n                    exclude_keys=[\"time\"],\n                )\n                if diff_keys:\n                    err_msg = \"The following dimensions differ: \" + \", \".join(\n                        sorted(diff_keys)\n                    )\n                    results[test][\"msgs\"][err_msg].append(consistency_files[file])\n\n                # Compare coordinates\n                test = \"Coordinates\"\n                results[test][\"weight\"] = 3\n                diff_keys = compare_dicts(\n                    reference_data[\"coordinates\"],\n                    data[\"coordinates\"],\n                    exclude_keys=excl_coords,\n                )\n                if diff_keys:\n                    err_msg = \"The following coordinates differ: \" + \", \".join(\n                        sorted(diff_keys)\n                    )\n                    results[test][\"msgs\"][err_msg].append(consistency_files[file])\n\n    return results\n</code></pre>"},{"location":"reference/esgf_qa_con_checks/#esgf_qa.con_checks.continuity_checks","title":"<code>continuity_checks(ds, ds_map, files_to_check_dict, checker_options)</code>","text":"<p>Checks inter-file time and time_bnds continuity for a dataset.</p> <p>This check identifies gaps in time or time_bnds between files of a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>str</code> <p>Dataset to process.</p> required <code>ds_map</code> <code>dict</code> <p>Dictionary mapping dataset IDs to file paths.</p> required <code>files_to_check_dict</code> <code>dict</code> <p>A special dictionary mapping files to check to datasets.</p> required <code>checker_options</code> <code>dict</code> <p>Dictionary of checker options.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of results.</p> Source code in <code>esgf_qa/con_checks.py</code> <pre><code>def continuity_checks(ds, ds_map, files_to_check_dict, checker_options):\n    \"\"\"\n    Checks inter-file time and time_bnds continuity for a dataset.\n\n    This check identifies gaps in time or time_bnds between files of a dataset.\n\n    Parameters\n    ----------\n    ds : str\n        Dataset to process.\n    ds_map : dict\n        Dictionary mapping dataset IDs to file paths.\n    files_to_check_dict : dict\n        A special dictionary mapping files to check to datasets.\n    checker_options : dict\n        Dictionary of checker options.\n\n    Returns\n    -------\n    dict\n        Dictionary of results.\n    \"\"\"\n    results = defaultdict(level1_factory)\n    filelist = sorted(ds_map[ds])\n    consistency_files = OrderedDict(\n        (files_to_check_dict[i][\"consistency_file\"], i) for i in filelist\n    )\n\n    # Check time and time_bnds continuity\n    test = \"Time continuity\"\n    results[test][\"weight\"] = 3\n    timen = None\n    boundn = None\n    i = 0\n    for file in consistency_files.keys():\n        with open(file) as fc:\n            data = json.load(fc)\n            i += 1\n            prev_timen = timen\n            prev_boundn = boundn\n            timen = (\n                cftime.num2date(\n                    data[\"time_info\"][\"timen\"],\n                    units=data[\"time_info\"][\"units\"],\n                    calendar=data[\"time_info\"][\"calendar\"],\n                )\n                if data[\"time_info\"][\"timen\"]\n                else None\n            )\n            boundn = (\n                cftime.num2date(\n                    data[\"time_info\"][\"boundn\"],\n                    units=data[\"time_info\"][\"units\"],\n                    calendar=data[\"time_info\"][\"calendar\"],\n                )\n                if data[\"time_info\"][\"boundn\"]\n                else None\n            )\n            if i == 1:\n                continue\n            time0 = (\n                cftime.num2date(\n                    data[\"time_info\"][\"time0\"],\n                    units=data[\"time_info\"][\"units\"],\n                    calendar=data[\"time_info\"][\"calendar\"],\n                )\n                if data[\"time_info\"][\"time0\"]\n                else None\n            )\n            bound0 = (\n                cftime.num2date(\n                    data[\"time_info\"][\"bound0\"],\n                    units=data[\"time_info\"][\"units\"],\n                    calendar=data[\"time_info\"][\"calendar\"],\n                )\n                if data[\"time_info\"][\"bound0\"]\n                else None\n            )\n            freq = data[\"time_info\"][\"frequency\"]\n            if (time0 or timen or bound0 or boundn) and not freq:\n                err_msg = \"Frequency could not be inferred\"\n                results[test][\"msgs\"][err_msg].append(consistency_files[file])\n                continue\n            elif (time0 or timen or bound0 or boundn) and freq not in deltdic:\n                err_msg = f\"Unsupported frequency '{freq}'\"\n                continue\n\n            if time0 and prev_timen:\n                delt = time0 - prev_timen\n                delts = delt.total_seconds()\n                if delts &gt; deltdic[freq + \"max\"] or delts &lt; deltdic[freq + \"min\"]:\n                    err_msg = f\"Gap in time axis (between files) - previous {prev_timen} - current {time0} - delta-t {printtimedelta(delts)}\"\n                    results[test][\"msgs\"][err_msg].append(consistency_files[file])\n\n            if bound0 and prev_boundn:\n                delt_bnd = bound0 - prev_boundn\n                delts_bnd = delt_bnd.total_seconds()\n                if delts_bnd &lt; -1:\n                    err_msg = f\"Overlapping time bounds (between files) - previous {prev_boundn} - current {bound0} - delta-t {printtimedelta(delts_bnd)}\"\n                    results[test][\"msgs\"][err_msg].append(consistency_files[file])\n                if delts_bnd &gt; 1:\n                    err_msg = f\"Gap in time bounds (between files) - previous {prev_boundn} - current {bound0} - delta-t {printtimedelta(delts_bnd)}\"\n                    results[test][\"msgs\"][err_msg].append(consistency_files[file])\n\n    return results\n</code></pre>"},{"location":"reference/esgf_qa_con_checks/#esgf_qa.con_checks.dataset_coverage_checks","title":"<code>dataset_coverage_checks(ds_map, files_to_check_dict, checker_options)</code>","text":"<p>Checks consistency of dataset time coverage.</p> <p>Variables that differ in their time coverage are reported.</p> <p>Parameters:</p> Name Type Description Default <code>ds_map</code> <code>dict</code> <p>Dictionary mapping dataset IDs to file paths.</p> required <code>files_to_check_dict</code> <code>dict</code> <p>A special dictionary mapping files to check to datasets.</p> required <code>checker_options</code> <code>dict</code> <p>Dictionary of checker options.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of results.</p> Source code in <code>esgf_qa/con_checks.py</code> <pre><code>def dataset_coverage_checks(ds_map, files_to_check_dict, checker_options):\n    \"\"\"\n    Checks consistency of dataset time coverage.\n\n    Variables that differ in their time coverage are reported.\n\n    Parameters\n    ----------\n    ds_map : dict\n        Dictionary mapping dataset IDs to file paths.\n    files_to_check_dict : dict\n        A special dictionary mapping files to check to datasets.\n    checker_options : dict\n        Dictionary of checker options.\n\n    Returns\n    -------\n    dict\n        Dictionary of results.\n    \"\"\"\n    results = defaultdict(level0_factory)\n    test = \"Time coverage\"\n\n    coverage_start = dict()\n    coverage_end = dict()\n\n    # Extract time coverage for each dataset\n    for ds in ds_map.keys():\n        fl = sorted(ds_map[ds])\n        ts0 = None\n        tsn = None\n        try:\n            if files_to_check_dict[fl[0]][\"ts\"] != \"\":\n                ts0 = files_to_check_dict[fl[0]][\"ts\"].split(\"-\")[0][0:4]\n                # If time interval of timestamp does not start in January, use following year\n                if len(files_to_check_dict[fl[-1]][\"ts\"].split(\"-\")[0]) &gt;= 6:\n                    if files_to_check_dict[fl[-1]][\"ts\"].split(\"-\")[0][4:6] != \"01\":\n                        coverage_start[ds] = int(ts0) + 1\n                    else:\n                        coverage_start[ds] = int(ts0)\n                coverage_start[ds] = int(ts0)\n            if files_to_check_dict[fl[-1]][\"ts\"] != \"\":\n                tsn = files_to_check_dict[fl[-1]][\"ts\"].split(\"-\")[1][0:4]\n                # If time interval of timestamp ends in January, use previous year\n                if len(files_to_check_dict[fl[-1]][\"ts\"].split(\"-\")[1]) &gt;= 6:\n                    if files_to_check_dict[fl[-1]][\"ts\"].split(\"-\")[1][4:6] == \"01\":\n                        coverage_end[ds] = int(tsn) - 1\n                    else:\n                        coverage_end[ds] = int(tsn)\n                else:\n                    coverage_end[ds] = int(tsn)\n            if ts0 is None and tsn is None:\n                continue\n            elif ts0 is None:\n                results[ds][test][\"weight\"] = 1\n                results[ds][test][\"msgs\"][\n                    \"Begin of time coverage cannot be inferred.\"\n                ] = [fl[0]]\n                continue\n            elif tsn is None:\n                results[ds][test][\"weight\"] = 1\n                results[ds][test][\"msgs\"][\n                    \"End of time coverage cannot be inferred.\"\n                ] = [fl[-1]]\n                continue\n        except IndexError or ValueError:\n            results[ds][test][\"weight\"] = 1\n            if len(fl) &gt; 1:\n                results[ds][test][\"msgs\"][\"Time coverage cannot be inferred.\"] = [\n                    fl[0],\n                    fl[-1],\n                ]\n            else:\n                results[ds][test][\"msgs\"][\"Time coverage cannot be inferred.\"] = [fl[0]]\n            continue\n\n    # Compare coverage\n    if len(coverage_start.keys()) &gt; 1:\n        try:\n            scov = min(coverage_start.values())\n        except ValueError:\n            scov = None\n        try:\n            ecov = max(coverage_end.values())\n        except ValueError:\n            ecov = None\n        # Get all ds where coverage_start differs\n        for ds in coverage_start.keys():\n            fl = sorted(ds_map[ds])\n            if scov is None:\n                pass\n            elif coverage_start[ds] != scov:\n                results[ds][test][\"weight\"] = 1\n                results[ds][test][\"msgs\"][\n                    f\"Time series starts at '{coverage_start[ds]}' while other time series start at '{scov}'\"\n                ] = [fl[0]]\n            if ecov is None:\n                pass\n            elif ds in coverage_end and coverage_end[ds] != ecov:\n                results[ds][test][\"weight\"] = 1\n                results[ds][test][\"msgs\"][\n                    f\"Time series ends at '{coverage_end[ds]}' while other time series end at '{ecov}'\"\n                ] = [fl[-1]]\n\n    return results\n</code></pre>"},{"location":"reference/esgf_qa_con_checks/#esgf_qa.con_checks.inter_dataset_consistency_checks","title":"<code>inter_dataset_consistency_checks(ds_map, files_to_check_dict, checker_options)</code>","text":"<p>Inter-dataset consistency checks.</p> <p>Will group datasets by realm and grid for certain checks. Runs inter-dataset consistency checks:</p> <pre><code>- Required and non-required global attributes (values and data types)\n- Coordinates (values)\n- Dimensions (names and sizes)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>ds_map</code> <code>dict</code> <p>Dictionary mapping dataset IDs to file paths.</p> required <code>files_to_check_dict</code> <code>dict</code> <p>A special dictionary mapping files to check to datasets.</p> required <code>checker_options</code> <code>dict</code> <p>Dictionary of checker options.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of results.</p> Source code in <code>esgf_qa/con_checks.py</code> <pre><code>def inter_dataset_consistency_checks(ds_map, files_to_check_dict, checker_options):\n    \"\"\"\n    Inter-dataset consistency checks.\n\n    Will group datasets by realm and grid for certain checks.\n    Runs inter-dataset consistency checks:\n\n        - Required and non-required global attributes (values and data types)\n        - Coordinates (values)\n        - Dimensions (names and sizes)\n\n    Parameters\n    ----------\n    ds_map : dict\n        Dictionary mapping dataset IDs to file paths.\n    files_to_check_dict : dict\n        A special dictionary mapping files to check to datasets.\n    checker_options : dict\n        Dictionary of checker options.\n\n    Returns\n    -------\n    dict\n        Dictionary of results.\n    \"\"\"\n    results = defaultdict(level0_factory)\n    filedict = {}\n    consistency_data = {}\n    for ds in ds_map.keys():\n        filedict[ds] = sorted(ds_map[ds])[0]\n\n    # Exclude the following global attributes from comparison\n    excl_global_attrs = [\n        \"creation_date\",\n        \"history\",\n        \"tracking_id\",\n        \"variable_id\",\n        \"frequency\",\n        \"external_variables\",\n        \"table_id\",\n        \"grid\",\n        \"grid_label\",\n        \"realm\",\n        \"modeling_realm\",\n    ]\n\n    # Include the following global attributes in the realm-specific comparison\n    incl_global_attrs = [\"grid\", \"grid_label\", \"realm\", \"modeling_realm\"]\n\n    # Consistency data\n    for ds, dsfile0 in filedict.items():\n        consistency_file = files_to_check_dict[dsfile0][\"consistency_file\"]\n        with open(consistency_file) as f:\n            data = json.load(f)\n            consistency_data[ds] = data\n\n    # Reference datasets\n    ref_ds = dict()\n\n    # Compare each file with reference\n    for ds, data in consistency_data.items():\n        # Select first dataset as main reference\n        if \"Main\" not in ref_ds:\n            ref_ds[\"Main\"] = ds\n        # Also group datasets by realm and grid label\n        #   for grid / realm specific consistency checks\n        realm = ChainMap(\n            data[\"global_attributes\"], data[\"global_attributes_non_required\"]\n        ).get(\"realm\", None)\n        if not realm:\n            realm = ChainMap(\n                data[\"global_attributes\"], data[\"global_attributes_non_required\"]\n            ).get(\"modeling_realm\", None)\n        if not realm:\n            realm = \"Default\"\n        gridlabel = ChainMap(\n            data[\"global_attributes\"], data[\"global_attributes_non_required\"]\n        ).get(\"grid_label\", None)\n        if not gridlabel:\n            gridlabel = ChainMap(\n                data[\"global_attributes\"], data[\"global_attributes_non_required\"]\n            ).get(\"grid\", None)\n        if not gridlabel:\n            gridlabel = \"Default\"\n        ref_ds_key = f\"{realm}/{gridlabel}\"\n        if ref_ds_key not in ref_ds:\n            ref_ds[ref_ds_key] = ds\n            continue\n        else:\n            reference_data_rg = consistency_data[ref_ds[ref_ds_key]]\n            reference_data = consistency_data[ref_ds[\"Main\"]]\n\n            # Compare required global attributes\n            test = \"Required global attributes (Inter-Dataset)\"\n            results[ds][test][\"weight\"] = 2\n            diff_keys = compare_dicts(\n                reference_data[\"global_attributes\"],\n                data[\"global_attributes\"],\n                exclude_keys=excl_global_attrs,\n            )\n            if diff_keys:\n                err_msg = (\n                    \"The following global attributes differ between datasets: \"\n                    + \", \".join(sorted(diff_keys))\n                )\n                results[ds][test][\"msgs\"][err_msg].append(filedict[ds])\n\n            # Compare specific global attributes\n            test = \"Realm-specific global attributes (Inter-Dataset)\"\n            results[ds][test][\"weight\"] = 2\n            diff_keys = compare_dicts(\n                {\n                    k: ChainMap(\n                        reference_data_rg[\"global_attributes\"],\n                        reference_data_rg[\"global_attributes_non_required\"],\n                    ).get(k, \"unset\")\n                    for k in incl_global_attrs\n                },\n                {\n                    k: ChainMap(\n                        data[\"global_attributes\"],\n                        data[\"global_attributes_non_required\"],\n                    ).get(k, \"unset\")\n                    for k in incl_global_attrs\n                },\n                exclude_keys=[],\n            )\n            if diff_keys:\n                err_msg = (\n                    f\"The following realm-specific global attributes differ between datasets (realm/grid_label: {truncate_str(ref_ds_key.split('/')[0])}/{truncate_str(ref_ds_key.split('/')[1])}): \"\n                    + \", \".join(sorted(diff_keys))\n                )\n                results[ds][test][\"msgs\"][err_msg].append(filedict[ds])\n\n            # Compare non-required global attributes\n            test = \"Non-required global attributes (Inter-Dataset)\"\n            results[ds][test][\"weight\"] = 1\n            diff_keys = compare_dicts(\n                reference_data[\"global_attributes_non_required\"],\n                data[\"global_attributes_non_required\"],\n                exclude_keys=excl_global_attrs,\n            )\n            if diff_keys:\n                err_msg = (\n                    \"The following non-required global attributes differ between datasets: \"\n                    + \", \".join(sorted(diff_keys))\n                )\n                results[ds][test][\"msgs\"][err_msg].append(filedict[ds])\n\n            # Compare global attributes dtypes\n            test = \"Global attributes data types (Inter-Dataset)\"\n            results[ds][test][\"weight\"] = 2\n            diff_keys = compare_dicts(\n                reference_data[\"global_attributes_dtypes\"],\n                data[\"global_attributes_dtypes\"],\n                exclude_keys=[],\n            )\n            if diff_keys:\n                err_msg = (\n                    \"The following global attributes have inconsistent data types between datasets: \"\n                    + \", \".join(sorted(diff_keys))\n                )\n                results[ds][test][\"msgs\"][err_msg].append(filedict[ds])\n\n            # Compare dimensions\n            test = \"Dimensions (Inter-Dataset)\"\n            results[ds][test][\"weight\"] = 2\n            diff_keys = compare_dicts(\n                reference_data_rg[\"dimensions\"],\n                data[\"dimensions\"],\n                exclude_keys=[\"time\", \"depth\", \"lev\"],\n            )\n            if diff_keys:\n                err_msg = (\n                    \"The following dimensions differ between datasets: \"\n                    + \", \".join(sorted(diff_keys))\n                )\n                results[ds][test][\"msgs\"][err_msg].append(filedict[ds])\n\n            # Compare coordinates\n            test = \"Coordinates (Inter-Dataset)\"\n            results[ds][test][\"weight\"] = 2\n            diff_keys = compare_dicts(\n                reference_data_rg[\"coordinates\"],\n                data[\"coordinates\"],\n                exclude_keys=[\n                    \"depth\",\n                    \"depth_bnds\",\n                    \"lev\",\n                    \"lev_bnds\",\n                    \"plev\",\n                    \"height\",\n                ],\n            )\n            if diff_keys:\n                err_msg = (\n                    \"The following coordinates differ between datasets: \"\n                    + \", \".join(sorted(diff_keys))\n                )\n                results[ds][test][\"msgs\"][err_msg].append(filedict[ds])\n\n    # List reference datasets\n    print(\"The following datasets were used as reference:\")\n    print(f\" - General reference: {ref_ds['Main']}\")\n    reference_datasets = {\"general_reference\": ref_ds[\"Main\"]}\n    for key in sorted(list(ref_ds.keys())):\n        if key == \"Main\":\n            continue\n        else:\n            reference_datasets[key] = ref_ds[key]\n            print(\n                f\" - '{truncate_str(key.split('/')[0])}' / '{truncate_str(key.split('/')[1])}' (realm / grid): {ref_ds[key]}\"\n            )\n\n    print()\n\n    return results, reference_datasets\n</code></pre>"},{"location":"reference/esgf_qa_con_checks/#esgf_qa.con_checks.printtimedelta","title":"<code>printtimedelta(d)</code>","text":"<p>Return timedelta (s) as either min, hours, days, whatever fits best.</p> Source code in <code>esgf_qa/con_checks.py</code> <pre><code>def printtimedelta(d):\n    \"\"\"Return timedelta (s) as either min, hours, days, whatever fits best.\"\"\"\n    if d &gt; 86000:\n        return f\"{d/86400.} days\"\n    if d &gt; 3500:\n        return f\"{d/3600.} hours\"\n    if d &gt; 50:\n        return f\"{d/60.} minutes\"\n    else:\n        return f\"{d} seconds\"\n</code></pre>"},{"location":"reference/esgf_qa_con_checks/#esgf_qa.con_checks.truncate_str","title":"<code>truncate_str(s, max_length=16)</code>","text":"<p>Truncate string if too long.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>String to truncate.</p> required <code>max_length</code> <code>int</code> <p>Maximum length of string. Default is 16.</p> <code>16</code> <p>Returns:</p> Type Description <code>str</code> <p>Truncated string.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; truncate_str(\"This is a long string\", 10)\n'This...string'\n&gt;&gt;&gt; truncate_str(\"This is a short string\", 16)\n'This is a short string'\n</code></pre> Source code in <code>esgf_qa/con_checks.py</code> <pre><code>def truncate_str(s, max_length=16):\n    \"\"\"\n    Truncate string if too long.\n\n    Parameters\n    ----------\n    s : str\n        String to truncate.\n    max_length : int, optional\n        Maximum length of string. Default is 16.\n\n    Returns\n    -------\n    str\n        Truncated string.\n\n    Examples\n    --------\n    &gt;&gt;&gt; truncate_str(\"This is a long string\", 10)\n    'This...string'\n    &gt;&gt;&gt; truncate_str(\"This is a short string\", 16)\n    'This is a short string'\n    \"\"\"\n    if max_length &lt;= 15 or len(s) &lt;= max_length:\n        return s\n\n    # Select start and end of string\n    words = s.split()\n    start = \"\"\n    end = \"\"\n\n    for i in range(len(words)):\n        if len(\" \".join(words[: i + 1])) &gt;= 6:\n            start = \" \".join(words[: i + 1])\n            break\n\n    for i in range(len(words) - 1, -1, -1):\n        if len(\" \".join(words[i:])) &gt;= 6:\n            end = \" \".join(words[i:])\n            break\n\n    # Return truncated string\n    if len(start) + len(end) + 3 &gt;= len(s):\n        return s\n    else:\n        return f\"{start}...{end}\"\n</code></pre>"},{"location":"reference/esgf_qa_qaviewer/","title":"QA Results Viewer <code>esgf_qa.qaviewer</code>","text":""},{"location":"reference/esgf_qa_qaviewer/#esgf_qa.qaviewer.QCViewer","title":"<code>QCViewer</code>","text":"<p>               Bases: <code>App</code></p> Source code in <code>esgf_qa/qaviewer.py</code> <pre><code>class QCViewer(App):\n    BINDINGS = [\n        (\"q\", \"quit\", \"Quit\"),\n        (\"/\", \"focus_search\", \"Search\"),\n        (\"n\", \"next_match\", \"Next Match\"),\n        (\"p\", \"prev_match\", \"Prev Match\"),\n    ]\n\n    def __init__(self, data):\n        super().__init__()\n        self.data = data\n        self.matches = []\n        self.match_index = -1\n        self.qc_tree = None  # will be created in compose()\n        self.last_highlighted_node = None\n\n    def compose(self) -&gt; ComposeResult:\n        yield Header()\n        self.qc_tree = Tree(\"QC Results\", id=\"qc-tree\")\n        yield self.qc_tree\n        yield Input(placeholder=\"Search...\", id=\"search\")\n        yield Static(\"\", id=\"status\")\n        yield QCFooter()\n\n    def on_tree_node_selected(self, event: Tree.NodeSelected):\n        # Right-click simulation: expand all children\n        # Unfortunately, Tree.NodeSelected does not carry button info\n        # We rely on a right-click flag set in on_mouse_down\n        if getattr(self, \"_right_click_pending\", False):\n            self.toggle_expand_node(event.node)\n            self._right_click_pending = False\n\n    def on_mouse_up(self, event: MouseUp):\n        # Set a flag if right-click\n        if event.button == 3:  # right click\n            self._right_click_pending = True\n            # node = self.qc_tree.cursor_node\n            # if node:\n            #    self.toggle_expand_node(node)\n\n    def toggle_expand_node(self, node):\n        \"\"\"Right-click: expand if collapsed, collapse if expanded, recursively.\"\"\"\n        if hasattr(node, \"_expanded_state\"):\n            # toggle previous state\n            expanding = not node._expanded_state\n        else:\n            # first time, check actual state\n            expanding = not node.is_expanded\n\n        if not expanding:\n            # Collapse node and all children\n            self._collapse_tree(node)\n        else:\n            # Expand node and all children, potentially only up to certain level\n            depth = self.get_node_depth(node)\n            if depth &lt;= 1:\n                self._expand_tree_up_to_depth(node, current_lvl=depth, target_lvl=2)\n            else:\n                self._expand_tree(node)\n\n    def get_node_depth(self, node):\n        \"\"\"Return depth of node (root=0).\"\"\"\n        depth = 0\n        parent = node.parent\n        while parent:\n            depth += 1\n            parent = parent.parent\n        return depth\n\n    def _expand_tree_up_to_depth(self, node, current_lvl, target_lvl):\n        \"\"\"Recursively expand node up to certain depth level.\"\"\"\n        node.expand()\n        node._expanded_state = True\n        if current_lvl &gt;= target_lvl:\n            return\n        for child in node.children:\n            self._expand_tree_up_to_depth(child, current_lvl + 1, target_lvl)\n\n    def _expand_tree(self, node):\n        \"\"\"Recursively expand node and all children.\"\"\"\n        node.expand()\n        node._expanded_state = True\n        for child in node.children:\n            self._expand_tree(child)\n\n    def _collapse_tree(self, node):\n        \"\"\"Recursively collapses node and all children.\"\"\"\n        for child in node.children:\n            self._collapse_tree(child)\n        node.collapse()\n        node._expanded_state = False\n\n    def on_mount(self):\n        self.populate_tree(self.qc_tree.root, self.data)\n        self.qc_tree.root.expand()\n\n    def populate_tree(self, node, data):\n        if isinstance(data, dict):\n            for k, v in data.items():\n                child = node.add(k, expand=False)\n                self.populate_tree(child, v)\n        elif isinstance(data, list):\n            for i, v in enumerate(data):\n                child = node.add(f\"[{i}]\", expand=False)\n                self.populate_tree(child, v)\n        else:\n            node.add(repr(data))\n\n    def action_focus_search(self):\n        self.query_one(\"#search\").focus()\n\n    def on_input_submitted(self, event: Input.Submitted):\n        \"\"\"Called when the user submits a search in the input.\"\"\"\n        query = event.value.strip()\n        self.matches = []\n        self.match_index = -1\n\n        if query:\n            # Collect all matching nodes\n            for node in iter_nodes(self.qc_tree.root):\n                if query.lower() in str(node.label).lower():\n                    self.matches.append(node)\n\n        if self.matches:\n            # Start at first match\n            self.match_index = 0\n            # Jump to it (expand path, collapse old if needed)\n            self.jump_to_match()\n            # Return focus to tree so n/p work\n            self.set_focus(self.qc_tree)\n        else:\n            self.query_one(\"#status\", Static).update(f\"No matches for '{query}'\")\n\n    def focus_match(self):\n        node = self.matches[self.match_index]\n        node.expand_all()\n        self.qc_tree.select_node(node)\n        self.qc_tree.scroll_to_node(node)\n        self.query_one(\"#status\", Static).update(\n            f\"Match {self.match_index+1}/{len(self.matches)}: {node.label}\"\n        )\n\n    def action_next_match(self):\n        if self.matches:\n            self.match_index = (self.match_index + 1) % len(self.matches)\n            self.jump_to_match()\n\n    def action_prev_match(self):\n        if self.matches:\n            self.match_index = (self.match_index - 1) % len(self.matches)\n            self.jump_to_match()\n\n    def jump_to_match(self) -&gt; None:\n        \"\"\"Jump to the current match, expanding its parents and collapsing previous.\"\"\"\n        if not self.matches or self.match_index &lt; 0:\n            return\n\n        # Collapse the previously focused node if any\n        if hasattr(self, \"current_match_node\") and self.current_match_node is not None:\n            try:\n                self.current_match_node.collapse()\n            except Exception:\n                pass\n\n        # Get the current match node\n        node = self.matches[self.match_index]\n        self.current_match_node = node\n\n        # Expand all parents so the node is visible\n        parent = node.parent\n        while parent:\n            parent.expand()\n            parent = parent.parent\n\n        # Expand this node itself too\n        node.expand()\n\n        # Scroll to and select it\n        self.qc_tree.select_node(node)\n        self.qc_tree.scroll_to_node(node)\n        self.qc_tree.select_node(node)\n\n        # Status line\n        self.query_one(\"#status\", Static).update(\n            f\"Match {self.match_index+1}/{len(self.matches)}: {node.label}\"\n        )\n</code></pre>"},{"location":"reference/esgf_qa_qaviewer/#esgf_qa.qaviewer.QCViewer.get_node_depth","title":"<code>get_node_depth(node)</code>","text":"<p>Return depth of node (root=0).</p> Source code in <code>esgf_qa/qaviewer.py</code> <pre><code>def get_node_depth(self, node):\n    \"\"\"Return depth of node (root=0).\"\"\"\n    depth = 0\n    parent = node.parent\n    while parent:\n        depth += 1\n        parent = parent.parent\n    return depth\n</code></pre>"},{"location":"reference/esgf_qa_qaviewer/#esgf_qa.qaviewer.QCViewer.jump_to_match","title":"<code>jump_to_match()</code>","text":"<p>Jump to the current match, expanding its parents and collapsing previous.</p> Source code in <code>esgf_qa/qaviewer.py</code> <pre><code>def jump_to_match(self) -&gt; None:\n    \"\"\"Jump to the current match, expanding its parents and collapsing previous.\"\"\"\n    if not self.matches or self.match_index &lt; 0:\n        return\n\n    # Collapse the previously focused node if any\n    if hasattr(self, \"current_match_node\") and self.current_match_node is not None:\n        try:\n            self.current_match_node.collapse()\n        except Exception:\n            pass\n\n    # Get the current match node\n    node = self.matches[self.match_index]\n    self.current_match_node = node\n\n    # Expand all parents so the node is visible\n    parent = node.parent\n    while parent:\n        parent.expand()\n        parent = parent.parent\n\n    # Expand this node itself too\n    node.expand()\n\n    # Scroll to and select it\n    self.qc_tree.select_node(node)\n    self.qc_tree.scroll_to_node(node)\n    self.qc_tree.select_node(node)\n\n    # Status line\n    self.query_one(\"#status\", Static).update(\n        f\"Match {self.match_index+1}/{len(self.matches)}: {node.label}\"\n    )\n</code></pre>"},{"location":"reference/esgf_qa_qaviewer/#esgf_qa.qaviewer.QCViewer.on_input_submitted","title":"<code>on_input_submitted(event)</code>","text":"<p>Called when the user submits a search in the input.</p> Source code in <code>esgf_qa/qaviewer.py</code> <pre><code>def on_input_submitted(self, event: Input.Submitted):\n    \"\"\"Called when the user submits a search in the input.\"\"\"\n    query = event.value.strip()\n    self.matches = []\n    self.match_index = -1\n\n    if query:\n        # Collect all matching nodes\n        for node in iter_nodes(self.qc_tree.root):\n            if query.lower() in str(node.label).lower():\n                self.matches.append(node)\n\n    if self.matches:\n        # Start at first match\n        self.match_index = 0\n        # Jump to it (expand path, collapse old if needed)\n        self.jump_to_match()\n        # Return focus to tree so n/p work\n        self.set_focus(self.qc_tree)\n    else:\n        self.query_one(\"#status\", Static).update(f\"No matches for '{query}'\")\n</code></pre>"},{"location":"reference/esgf_qa_qaviewer/#esgf_qa.qaviewer.QCViewer.toggle_expand_node","title":"<code>toggle_expand_node(node)</code>","text":"<p>Right-click: expand if collapsed, collapse if expanded, recursively.</p> Source code in <code>esgf_qa/qaviewer.py</code> <pre><code>def toggle_expand_node(self, node):\n    \"\"\"Right-click: expand if collapsed, collapse if expanded, recursively.\"\"\"\n    if hasattr(node, \"_expanded_state\"):\n        # toggle previous state\n        expanding = not node._expanded_state\n    else:\n        # first time, check actual state\n        expanding = not node.is_expanded\n\n    if not expanding:\n        # Collapse node and all children\n        self._collapse_tree(node)\n    else:\n        # Expand node and all children, potentially only up to certain level\n        depth = self.get_node_depth(node)\n        if depth &lt;= 1:\n            self._expand_tree_up_to_depth(node, current_lvl=depth, target_lvl=2)\n        else:\n            self._expand_tree(node)\n</code></pre>"},{"location":"reference/esgf_qa_qaviewer/#esgf_qa.qaviewer.iter_nodes","title":"<code>iter_nodes(node)</code>","text":"<p>Recursively yield all nodes starting from this one.</p> Source code in <code>esgf_qa/qaviewer.py</code> <pre><code>def iter_nodes(node):\n    \"\"\"Recursively yield all nodes starting from this one.\"\"\"\n    yield node\n    for child in node.children:\n        yield from iter_nodes(child)\n</code></pre>"},{"location":"reference/esgf_qa_qaviewer/#esgf_qa.qaviewer.transform_keys","title":"<code>transform_keys(data)</code>","text":"<p>Apply similar renaming logic than in display_qc_results.html</p> Source code in <code>esgf_qa/qaviewer.py</code> <pre><code>def transform_keys(data):\n    \"\"\"Apply similar renaming logic than in display_qc_results.html\"\"\"\n    weight_map = {3: \"Required\", 2: \"Recommended\", 1: \"Suggested\"}\n    result = {}\n    info_map = {\n        \"id\": \"Dataset-ID\",\n        \"date\": \"Date\",\n        \"parent_dir\": \"Root Directory\",\n        \"files\": \"# Files\",\n        \"datasets\": \"# Datasets\",\n        \"cc_version\": \"Compliance Checker Version\",\n        \"checkers\": \"Applied Checkers\",\n        \"inter_ds_con_checks_ref\": \"Reference Datasets (inter-dataset consistency checks)\",\n    }\n\n    if \"info\" in data and data[\"info\"]:\n        result[\"Info\"] = {}\n        for key, val in info_map.items():\n            result[\"Info\"][val] = data[\"info\"].get(key, \"UNSPECIFIED\")\n\n    if \"error\" in data and data[\"error\"]:\n        result[\"Runtime Errors\"] = data[\"error\"]\n\n    if \"fail\" in data and data[\"fail\"]:\n        fail_section = {}\n        for w, name in weight_map.items():\n            fail_section[name] = data[\"fail\"].get(str(w), data[\"fail\"].get(w, {}))\n        result[\"Failed Checks\"] = fail_section\n\n    if \"pass\" in data and data[\"pass\"]:\n        pass_section = {}\n        for w, name in weight_map.items():\n            pass_section[name] = data[\"pass\"].get(str(w), data[\"pass\"].get(w, {}))\n        result[\"Passed Checks\"] = pass_section\n\n    return result\n</code></pre>"},{"location":"reference/esgf_qa_run_qa/","title":"Run QA Workflow <code>esgf_qa.run_qa</code>","text":""},{"location":"reference/esgf_qa_run_qa/#esgf_qa.run_qa.get_checker_release_versions","title":"<code>get_checker_release_versions(checkers, checker_options={})</code>","text":"<p>Get the release versions of the checkers.</p> <p>Parameters:</p> Name Type Description Default <code>checkers</code> <code>list</code> <p>A list of checkers to get the release versions for.</p> required <code>checker_options</code> <code>dict</code> <p>A dictionary of options for the checkers. Example format: {\"cf\": {\"check_dimension_order\": True}}</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>Updates the global dictionary <code>checker_release_versions</code>.</p> Source code in <code>esgf_qa/run_qa.py</code> <pre><code>def get_checker_release_versions(checkers, checker_options={}):\n    \"\"\"\n    Get the release versions of the checkers.\n\n    Parameters\n    ----------\n    checkers : list\n        A list of checkers to get the release versions for.\n    checker_options : dict, optional\n        A dictionary of options for the checkers.\n        Example format: {\"cf\": {\"check_dimension_order\": True}}\n\n    Returns\n    -------\n    None\n        Updates the global dictionary ``checker_release_versions``.\n    \"\"\"\n    global checker_release_versions\n    global checker_dict\n    global checker_dict_ext\n    check_suite = CheckSuite(options=checker_options)\n    check_suite.load_all_available_checkers()\n    for checker in checkers:\n        if checker.split(\":\")[0] not in checker_release_versions:\n            if checker.split(\":\")[0] in checker_dict:\n                checker_release_versions[checker.split(\":\")[0]] = (\n                    check_suite.checkers.get(\n                        checker, \"unknown version\"\n                    )._cc_spec_version\n                )\n            elif checker.split(\":\")[0] in checker_dict_ext:\n                checker_release_versions[checker.split(\":\")[0]] = version\n</code></pre>"},{"location":"reference/esgf_qa_run_qa/#esgf_qa.run_qa.get_default_result_dir","title":"<code>get_default_result_dir()</code>","text":"<p>Get the default result directory.</p> <p>Returns:</p> Type Description <code>str</code> <p>Default result directory.</p> Source code in <code>esgf_qa/run_qa.py</code> <pre><code>def get_default_result_dir():\n    \"\"\"\n    Get the default result directory.\n\n    Returns\n    -------\n    str\n        Default result directory.\n    \"\"\"\n    global _timestamp\n    global _timestamp_with_ms\n    hash_object = hashlib.md5(_timestamp_with_ms.encode())\n    return (\n        os.path.abspath(\".\")\n        + f\"/esgf-qa-results_{_timestamp_filename}_{hash_object.hexdigest()}\"\n    )\n</code></pre>"},{"location":"reference/esgf_qa_run_qa/#esgf_qa.run_qa.get_dsid","title":"<code>get_dsid(files_to_check_dict, dataset_files_map_ext, file_path, project_id)</code>","text":"<p>Get the dataset id for a file.</p> <p>Parameters:</p> Name Type Description Default <code>files_to_check_dict</code> <code>dict</code> <p>Dictionary of files to check.</p> required <code>dataset_files_map_ext</code> <code>dict</code> <p>Dictionary of dataset files.</p> required <code>file_path</code> <code>str</code> <p>Path to the file.</p> required <code>project_id</code> <code>str</code> <p>Project id.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Dataset id.</p> Source code in <code>esgf_qa/run_qa.py</code> <pre><code>def get_dsid(files_to_check_dict, dataset_files_map_ext, file_path, project_id):\n    \"\"\"\n    Get the dataset id for a file.\n\n    Parameters\n    ----------\n    files_to_check_dict : dict\n        Dictionary of files to check.\n    dataset_files_map_ext : dict\n        Dictionary of dataset files.\n    file_path : str\n        Path to the file.\n    project_id : str\n        Project id.\n\n    Returns\n    -------\n    str\n        Dataset id.\n    \"\"\"\n    dir_id = files_to_check_dict[file_path][\"id_dir\"].split(\"/\")\n    fn_id = files_to_check_dict[file_path][\"id_fn\"].split(\"_\")\n    if project_id in dir_id:\n        last_index = len(dir_id) - 1 - dir_id[::-1].index(project_id)\n        dsid = \".\".join(dir_id[last_index:])\n    else:\n        dsid = \".\".join(dir_id)\n    if len(dataset_files_map_ext[files_to_check_dict[file_path][\"id_dir\"]].keys()) &gt; 1:\n        dsid += \".\" + \".\".join(fn_id)\n    return dsid\n</code></pre>"},{"location":"reference/esgf_qa_run_qa/#esgf_qa.run_qa.main","title":"<code>main()</code>","text":"<p>CLI entry point.</p> Source code in <code>esgf_qa/run_qa.py</code> <pre><code>def main():\n    \"\"\"\n    CLI entry point.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Run QA checks\")\n    parser.add_argument(\n        \"parent_dir\",\n        type=str,\n        help=\"Parent directory to scan for files\",\n        nargs=\"?\",\n    )\n    parser.add_argument(\n        \"-o\",\n        \"--output_dir\",\n        type=str,\n        default=get_default_result_dir(),\n        help=\"Directory to store QA results. Needs to be non-existing or empty or from previous QA run.\",\n    )\n    parser.add_argument(\n        \"-O\",\n        \"--option\",\n        default=[],\n        action=\"append\",\n        help=\"Additional options to be passed to the checkers. Format: '&lt;checker&gt;:&lt;option_name&gt;[:&lt;option_value&gt;]'. Multiple invocations possible.\",\n    )\n    parser.add_argument(\n        \"-t\",\n        \"--test\",\n        action=\"append\",\n        help=\"The test to run ('cc6:latest' or 'cf:&lt;version&gt;', can be specified multiple times, eg.: '-t cc6:latest -t cf:1.8') - default: running 'cc6:latest' and 'cf:1.11'.\",\n    )\n    parser.add_argument(\n        \"-i\",\n        \"--info\",\n        type=str,\n        help=\"Informtaion to be included in the QA results identifying the current run, eg. the experiment_id.\",\n    )\n    parser.add_argument(\n        \"-r\",\n        \"--resume\",\n        action=\"store_true\",\n        help=\"Specify to continue a previous QC run. Requires the &lt;output_dir&gt; argument to be set.\",\n    )\n    parser.add_argument(\n        \"-C\",\n        \"--include_consistency_checks\",\n        action=\"store_true\",\n        help=\"Include basic consistency and continuity checks. Default: False.\",\n    )\n    args = parser.parse_args()\n\n    result_dir = os.path.abspath(args.output_dir)\n    parent_dir = os.path.abspath(args.parent_dir) if args.parent_dir else None\n    tests = sorted(args.test) if args.test else []\n    info = args.info if args.info else \"\"\n    resume = args.resume\n    include_consistency_checks = (\n        args.include_consistency_checks if args.include_consistency_checks else False\n    )\n    cl_checker_options = parse_options(args.option)\n\n    # Progress file to track already checked files\n    progress_file = Path(result_dir, \"progress.txt\")\n    # Progress file to track already checked datasets\n    dataset_file = Path(result_dir, \"progress_datasets.txt\")\n\n    # Resume information stored in a json file\n    resume_info_file = Path(result_dir, \".resume_info\")\n\n    # Deal with result_dir\n    if not os.path.exists(result_dir):\n        if resume:\n            resume = False\n            warnings.warn(\n                \"Resume is set but specified output_directory does not exist. Starting a new QA run...\"\n            )\n        os.mkdir(result_dir)\n    elif os.listdir(result_dir) != []:\n        if resume:\n            required_files = [progress_file, resume_info_file]\n            required_paths = [os.path.join(result_dir, p) for p in [\"tables\"]]\n            if not all(os.path.isfile(rfile) for rfile in required_files) or not all(\n                os.path.isdir(rpath) for rpath in required_paths\n            ):\n                raise Exception(\n                    \"Resume is set but specified output_directory cannot be identified as output_directory of a previous QA run.\"\n                )\n        else:\n            if \"progress.txt\" in os.listdir(\n                result_dir\n            ) and \".resume_info\" in os.listdir(result_dir):\n                raise Exception(\n                    \"Specified output_directory is not empty but can be identified as output_directory of a previous QA run. Use'-r' or '--resume' (together with '-o' or '--output_dir') to continue the previous QA run or choose a different output_directory instead.\"\n                )\n            else:\n                raise Exception(\"Specified output_directory is not empty.\")\n    else:\n        if resume:\n            resume = False\n            warnings.warn(\n                \"Resume is set but specified output_directory is empty. Starting a new QA run...\"\n            )\n    if resume:\n        print(f\"Resuming previous QA run in '{result_dir}'\")\n        with open(os.path.join(result_dir, \".resume_info\")) as f:\n            try:\n                resume_info = json.load(f)\n                required_keys = [\"parent_dir\", \"info\", \"tests\"]\n                if not all(key in resume_info for key in required_keys):\n                    raise Exception(\n                        \"Invalid .resume_info file. It should contain the keys 'parent_dir', 'info', and 'tests'.\"\n                    )\n                if not (\n                    isinstance(resume_info[\"parent_dir\"], str)\n                    and isinstance(resume_info[\"info\"], str)\n                    and isinstance(resume_info[\"tests\"], list)\n                    and all(isinstance(test, str) for test in resume_info[\"tests\"])\n                ):\n                    raise Exception(\n                        \"Invalid .resume_info file. 'parent_dir' and 'info' should be strings, and 'tests' should be a list of strings.\"\n                    )\n            except json.JSONDecodeError:\n                raise Exception(\n                    \"Invalid .resume_info file. It should be a valid JSON file.\"\n                )\n            if tests and sorted(tests) != resume_info[\"tests\"]:\n                raise Exception(\"Cannot resume a previous QA run with different tests.\")\n            else:\n                tests = resume_info[\"tests\"]\n            if info and info != resume_info[\"info\"]:\n                warnings.warn(\n                    f\"&lt;info&gt; argument differs from the originally specified &lt;info&gt; argument ('{resume_info['info']}'). Using the new specification.\"\n                )\n            if parent_dir is None:\n                parent_dir = resume_info[\"parent_dir\"]\n            if parent_dir and Path(parent_dir) != Path(resume_info[\"parent_dir\"]):\n                raise Exception(\n                    \"Cannot resume a previous QA run with different &lt;parent_dir&gt;.\"\n                )\n            if cl_checker_options and cl_checker_options != resume_info.get(\n                \"checker_options\", {}\n            ):\n                raise Exception(\n                    \"Cannot resume a previous QA run with different &lt;option&gt; arguments.\"\n                )\n            else:\n                parent_dir = Path(resume_info[\"parent_dir\"])\n            if \"include_consistency_checks\" in resume_info:\n                include_consistency_checks = resume_info[\"include_consistency_checks\"]\n    else:\n        print(f\"Storing check results in '{result_dir}'\")\n\n    # Deal with tests\n    if not tests:\n        checkers = [\"cc6\", \"cf\"]\n        checkers_versions = {\"cc6\": \"latest\", \"cf\": \"1.11\"}\n        checker_options = defaultdict(dict)\n    else:\n        test_regex = re.compile(r\"^[a-z0-9_]+:(latest|[0-9]+(\\.[0-9]+)*)$\")\n        if not all([test_regex.match(test) for test in tests]):\n            raise Exception(\n                f\"Invalid test(s) specified. Please specify tests in the format 'checker_name:version'. Currently supported are: {', '.join(list(checker_dict.keys()))}, eerie.\"\n            )\n        checkers = [test.split(\":\")[0] for test in tests]\n        if sorted(checkers) != sorted(list(set(checkers))):\n            raise Exception(\"Cannot specify multiple instances of the same checker.\")\n        checkers_versions = {\n            test.split(\":\")[0]: (\n                test.split(\":\")[1]\n                if len(test.split(\":\")) == 2 and test.split(\":\")[1] != \"\"\n                else \"latest\"\n            )\n            for test in tests\n        }\n        checker_options = defaultdict(dict)\n        if \"cc6\" in checkers_versions and checkers_versions[\"cc6\"] != \"latest\":\n            checkers_versions[\"cc6\"] = \"latest\"\n            warnings.warn(\"Version of checker 'cc6' must be 'latest'. Using 'latest'.\")\n        if \"mip\" in checkers_versions and checkers_versions[\"mip\"] != \"latest\":\n            checkers_versions[\"mip\"] = \"latest\"\n            warnings.warn(\"Version of checker 'mip' must be 'latest'. Using 'latest'.\")\n            if \"tables\" not in cl_checker_options[\"mip\"]:\n                raise Exception(\n                    \"Option 'tables' with path to CMOR tables as value must be specified for checker 'mip'.\"\n                )\n        # EERIE support - hard code\n        if \"eerie\" in checkers_versions:\n            checkers_versions[\"mip\"] = \"latest\"\n            del checkers_versions[\"eerie\"]\n            if \"tables\" in cl_checker_options[\"eerie\"]:\n                cl_checker_options[\"mip\"][\"tables\"] = cl_checker_options[\"eerie\"][\n                    \"tables\"\n                ]\n            elif \"tables\" not in cl_checker_options[\"mip\"]:\n                cl_checker_options[\"mip\"][\n                    \"tables\"\n                ] = \"/work/bm0021/cmor_tables/eerie_cmor_tables/Tables\"\n        if sum(1 for ci in checkers_versions if ci in [\"mip\", \"cc6\"]) &gt; 1:\n            raise Exception(\n                \"ERROR: Cannot run both 'cc6' and 'mip' checkers at the same time.\"\n            )\n        if any(test not in checker_dict.keys() for test in checkers_versions):\n            raise Exception(\n                f\"Invalid test(s) specified. Supported are: {', '.join(checker_dict.keys())}\"\n            )\n\n    # Combine checkers and versions\n    #  (checker_options are hardcoded)\n    checkers = sorted([f\"{c}:{v}\" for c, v in checkers_versions.items()])\n\n    # Does parent_dir exist?\n    if parent_dir is None:\n        parser.error(\"Missing required argument &lt;parent_dir&gt;.\")\n    elif not os.path.exists(parent_dir):\n        raise Exception(f\"The specified &lt;parent_dir&gt; '{parent_dir}' does not exist.\")\n\n    # Write resume file\n    resume_info = {\n        \"parent_dir\": str(parent_dir),\n        \"info\": info,\n        \"tests\": checkers,\n    }\n    if include_consistency_checks:\n        resume_info[\"include_consistency_checks\"] = True\n    if cl_checker_options:\n        resume_info[\"checker_options\"] = cl_checker_options\n    with open(os.path.join(result_dir, \".resume_info\"), \"w\") as f:\n        json.dump(resume_info, f)\n\n    # If only cf checker is selected, run cc6 time checks only\n    if (\n        not any(cn.startswith(\"cc6\") or cn.startswith(\"mip\") for cn in checkers)\n        and include_consistency_checks\n    ):\n        time_checks_only = True\n        checkers.append(\"mip:latest\")\n        checkers.sort()\n    else:\n        time_checks_only = False\n\n    # Ensure progress files exist\n    os.makedirs(result_dir + \"/tables\", exist_ok=True)\n    progress_file.touch()\n    dataset_file.touch()\n\n    DRS_parent = \"CORDEX-CMIP6\"\n    for cname in checkers:\n        print(cname)\n        DRS_parent_tmp = DRS_path_parent.get(\n            checker_dict.get(cname.split(\":\")[0], \"\"), \"\"\n        )\n        if DRS_parent_tmp:\n            DRS_parent = DRS_parent_tmp\n            break\n\n    # Check if progress files exist and read already processed files/datasets\n    processed_files = set()\n    with open(progress_file) as file:\n        for line in file:\n            processed_files.add(line.strip())\n    processed_datasets = set()\n    with open(dataset_file) as file:\n        for line in file:\n            processed_datasets.add(line.strip())\n\n    # todo: allow black-/whitelisting (parts of) paths for checks\n    path_whitelist = []\n    path_blacklist = []\n\n    #########################################################\n    # Find all files to check and group them in datasets\n    #########################################################\n    files_to_check = []  # List of files to check\n    files_to_check_dict = {}\n    dataset_files_map = {}  # Map to store files grouped by their dataset_ids\n    dataset_files_map_ext = (\n        {}\n    )  # allowing files of multiple datasets in a single directory\n    for root, _, files in os.walk(parent_dir):\n        for file in files:\n            if file.endswith(\".nc\"):\n                file_path = os.path.normpath(os.path.join(root, file))\n                dataset_id_dir = os.path.dirname(file_path)\n                dataset_id_fn = \"_\".join(\n                    filter(\n                        re.compile(r\"^(?!\\d{1,}-{0,1}\\d{0,}$)\").match,\n                        os.path.splitext(os.path.basename(file_path))[0].split(\"_\"),\n                    )\n                )\n                dataset_timestamp = \"_\".join(\n                    filter(\n                        re.compile(r\"^\\d{1,}-?\\d*$\").match,\n                        os.path.splitext(os.path.basename(file_path))[0].split(\"_\"),\n                    )\n                )\n                os.makedirs(result_dir + dataset_id_dir + \"/result\", exist_ok=True)\n                os.makedirs(\n                    result_dir + dataset_id_dir + \"/consistency-output\", exist_ok=True\n                )\n                result_file = (\n                    result_dir\n                    + dataset_id_dir\n                    + \"/\"\n                    + \"result\"\n                    + \"/\"\n                    + dataset_id_fn\n                    + \"__\"\n                    + dataset_timestamp\n                    + \".json\"\n                )\n                consistency_file = (\n                    result_dir\n                    + dataset_id_dir\n                    + \"/\"\n                    + \"consistency-output\"\n                    + \"/\"\n                    + dataset_id_fn\n                    + \"__\"\n                    + dataset_timestamp\n                    + \".json\"\n                )\n                if \"_\" in dataset_timestamp:\n                    raise Exception(\n                        f\"Filename contains multiple time stamps: '{file_path}'\"\n                    )\n                if any(file_path.startswith(skip_path) for skip_path in path_blacklist):\n                    continue\n                if path_whitelist != [] and not any(\n                    file_path.startswith(use_path) for use_path in path_whitelist\n                ):\n                    continue\n                files_to_check.append(file_path)\n                files_to_check_dict[file_path] = {\n                    \"id_dir\": dataset_id_dir,\n                    \"id_fn\": dataset_id_fn,\n                    \"ts\": dataset_timestamp,\n                    \"result_file\": result_file,\n                    \"consistency_file\": consistency_file,\n                }\n                if dataset_id_dir in dataset_files_map_ext:\n                    if dataset_id_fn in dataset_files_map_ext[dataset_id_dir]:\n                        dataset_files_map_ext[dataset_id_dir][dataset_id_fn].append(\n                            file_path\n                        )\n                    else:\n                        dataset_files_map_ext[dataset_id_dir][dataset_id_fn] = [\n                            file_path\n                        ]\n                else:\n                    dataset_files_map_ext[dataset_id_dir] = {dataset_id_fn: [file_path]}\n    files_to_check = sorted(files_to_check)\n    for file_path in files_to_check:\n        files_to_check_dict[file_path][\"id\"] = get_dsid(\n            files_to_check_dict, dataset_files_map_ext, file_path, DRS_parent\n        )\n        files_to_check_dict[file_path][\"result_file_ds\"] = (\n            result_dir\n            + \"/\"\n            + files_to_check_dict[file_path][\"id_dir\"]\n            + \"/\"\n            + hashlib.md5(files_to_check_dict[file_path][\"id\"].encode()).hexdigest()\n            + \".json\"\n        )\n        if files_to_check_dict[file_path][\"id\"] in dataset_files_map:\n            dataset_files_map[files_to_check_dict[file_path][\"id\"]].append(file_path)\n        else:\n            dataset_files_map[files_to_check_dict[file_path][\"id\"]] = [file_path]\n        checker_options[file_path] = {\n            \"mip\": {\n                **cl_checker_options[\"mip\"],\n                \"consistency_output\": files_to_check_dict[file_path][\n                    \"consistency_file\"\n                ],\n                \"time_checks_only\": time_checks_only,\n            },\n            \"cc6\": {\n                **cl_checker_options[\"cc6\"],\n                \"consistency_output\": files_to_check_dict[file_path][\n                    \"consistency_file\"\n                ],\n                \"tables_dir\": result_dir + \"/tables\",\n                \"force_table_download\": file_path == files_to_check[0]\n                and (\n                    not resume or (resume and os.listdir(result_dir + \"/tables\") == [])\n                ),\n                \"time_checks_only\": time_checks_only,\n            },\n            \"cf:\": {\n                **cl_checker_options[\"cf\"],\n                \"enable_appendix_a_checks\": True,\n            },\n            \"wcrp_cmip6\": {\n                **cl_checker_options.get(\"wcrp_cmip6\", {}),\n                \"consistency_output\": files_to_check_dict[file_path][\n                    \"consistency_file\"\n                ],\n            },\n            \"wcrp_cordex_cmip6\": {\n                **cl_checker_options.get(\"wcrp_cordex_cmip6\", {}),\n                \"consistency_output\": files_to_check_dict[file_path][\n                    \"consistency_file\"\n                ],\n                \"tables_dir\": result_dir + \"/tables\",\n                \"force_table_download\": file_path == files_to_check[0]\n                and (\n                    not resume or (resume and os.listdir(result_dir + \"/tables\") == [])\n                ),\n            },\n        }\n        checker_options[file_path].update(\n            {\n                k: v\n                for k, v in cl_checker_options.items()\n                if k not in [\"cc6\", \"cf\", \"mip\"]\n            }\n        )\n\n    if len(files_to_check) == 0:\n        raise Exception(\"No files found to check.\")\n    else:\n        print(\n            f\"Found {len(files_to_check)} files (organized in {len(dataset_files_map)} datasets) to check.\"\n        )\n\n    print()\n    print(\"Files to check:\")\n    print(json.dumps(files_to_check, indent=4))\n    print()\n    print(\"Dataset - Files mapping (extended):\")\n    print(json.dumps(dataset_files_map_ext, indent=4))\n    print()\n    print(\"Dataset - Files mapping:\")\n    print(json.dumps(dataset_files_map, indent=4))\n    print()\n    print(\"Files to check dict:\")\n    print(json.dumps(files_to_check_dict, indent=4))\n    print()\n\n    #########################################################\n    # QA Part 1 - Run all compliance-checker checks\n    #########################################################\n\n    print()\n    print(\"#\" * 50)\n    print(\"# QA Part 1 - Run all compliance-checker checks\")\n    print(\"#\" * 50)\n    print()\n\n    # Initialize the summary\n    summary = QAResultAggregator()\n\n    # Calculate the number of processes\n    num_processes = max(multiprocessing.cpu_count() - 4, 1)\n    print(f\"Using {num_processes} parallel processes for cc checks.\")\n    print()\n\n    # Run the first process:\n    if len(files_to_check) &gt; 0:\n        processed_file, result_first = process_file(\n            files_to_check[0],\n            checkers,\n            checker_options[files_to_check[0]],\n            files_to_check_dict,\n            processed_files,\n            progress_file,\n        )\n        summary.update(\n            result_first, files_to_check_dict[processed_file][\"id\"], processed_file\n        )\n        del result_first\n\n    # Run the rest of the processes\n    if len(files_to_check) &gt; 1:\n        # Prepare the argument tuples\n        args = [\n            (\n                x,\n                checkers,\n                checker_options[x],\n                files_to_check_dict,\n                processed_files,\n                progress_file,\n            )\n            for x in files_to_check[1:]\n        ]\n\n        # Use a pool of workers to run jobs in parallel\n        with multiprocessing.Pool(processes=num_processes, maxtasksperchild=10) as pool:\n            # results = [result_first] + pool.starmap(\n            #    process_file, args\n            # )  # This collects all results in a list\n            for processed_file, result in pool.imap_unordered(call_process_file, args):\n                summary.update(\n                    result, files_to_check_dict[processed_file][\"id\"], processed_file\n                )\n                del result\n\n    # Skip continuity and consistency checks if no cc6/mip checks were run\n    #   (and thus no consistency output file was created)\n    if (\n        \"cc6:latest\" in checkers\n        or \"mip:latest\" in checkers\n        or \"wcrp_cmip6:1.0\" in checkers\n        or \"wcrp_cmip6:latest\" in checkers\n        or \"wcrp_cordex_cmip6:1.0\" in checkers\n        or \"wcrp_cordex_cmip6:latest\" in checkers\n    ):\n        #########################################################\n        # QA Part 2 - Run all consistency &amp; continuity checks\n        #########################################################\n\n        print()\n        print(\"#\" * 50)\n        print(\"# QA Part 2 - Run consistency &amp; continuity checks\")\n        print(\"#\" * 50)\n        print()\n\n        ###########################\n        # Consistency across files\n        print(\n            \"# QA Part 2.1 - Continuity &amp; Consistency across files of a single dataset\"\n        )\n        print(\n            \"#   (Reference for consistency checks is the first file of each respective dataset timeseries)\"\n        )\n        print()\n\n        # Calculate the number of processes\n        num_processes = max(multiprocessing.cpu_count() - 4, 1)\n        # Limit the number of processes for consistency checks since a lot\n        #   of files will be opened at the same time\n        num_processes = min(num_processes, 10)\n        print(f\"Using {num_processes} parallel processes for dataset checks.\")\n        print()\n\n        datasets = sorted(list(dataset_files_map.keys()))\n        args = [\n            (\n                x,\n                dataset_files_map,\n                [\"cons\", \"cont\", \"comp\"],\n                {\"cons\": {}, \"cont\": {}, \"comp\": {}},\n                files_to_check_dict,\n                processed_datasets,\n                dataset_file,\n            )\n            for x in datasets\n            if len(dataset_files_map[x]) &gt; 1\n        ]\n        if len(args) &gt; 0:\n            # Use a pool of workers to run jobs in parallel\n            with multiprocessing.Pool(\n                processes=num_processes, maxtasksperchild=10\n            ) as pool:\n                for processed_ds, result in pool.imap_unordered(\n                    call_process_dataset, args\n                ):\n                    summary.update_ds(result, processed_ds)\n                    del result\n\n        ##############################\n        # Consistency across datasets\n        print()\n        print(\"# QA Part 2.2 - Continuity &amp; Consistency across all datasets\")\n        print()\n\n        # Attributes and Coordinates\n        results_extra, reference_ds_dict = inter_dataset_consistency_checks(\n            dataset_files_map, files_to_check_dict, checker_options={}\n        )\n        for ds in results_extra.keys():\n            summary.update_ds({\"cons\": results_extra[ds]}, ds)\n\n        # Time coverage\n        results_extra = dataset_coverage_checks(\n            dataset_files_map, files_to_check_dict, checker_options={}\n        )\n        for ds in results_extra.keys():\n            summary.update_ds({\"cons\": results_extra[ds]}, ds)\n    else:\n        print()\n        warnings.warn(\n            \"Continuity &amp; Consistency checks skipped since no cc6 checks were run.\"\n        )\n\n    #########################################################\n    # Summarize and save results\n    #########################################################\n\n    print()\n    print(\"#\" * 50)\n    print(\n        f\"# QA Part {'3' if 'cc6:latest' in checkers or 'mip:latest' in checkers else '2'} - Summarizing and clustering the results\"\n    )\n    print(\"#\" * 50)\n    print()\n\n    # todo: always the latest checker version is used atm, but the\n    #       specified version should be used (\"tests\")\n    summary.sort()\n    qc_summary = summary.summary\n    get_checker_release_versions(checkers)\n    summary_info = {\n        \"id\": \"\",\n        \"date\": _timestamp_pprint,\n        \"files\": str(len(files_to_check)),\n        \"datasets\": str(len(dataset_files_map)),\n        \"cc_version\": cc_version,\n        \"checkers\": \", \".join(\n            [\n                f\"{checker_dict.get(checker.split(':')[0], '')} {checker.split(':')[0]}:{checker_release_versions[checker.split(':')[0]]}\"\n                for checker in checkers\n            ]\n        ),\n        \"parent_dir\": str(parent_dir),\n    }\n    # Add reference datasets for inter-dataset consistency checks\n    if \"cc6:latest\" in checkers or \"mip:latest\" in checkers:\n        summary_info[\"inter_ds_con_checks_ref\"] = reference_ds_dict\n\n    dsid_common_prefix = os.path.commonprefix(list(dataset_files_map.keys()))\n    if dsid_common_prefix != list(dataset_files_map.keys())[0]:\n        dsid_common_prefix = dsid_common_prefix + \"*\"\n    if info:\n        summary_info[\"id\"] = f\"{info} ({dsid_common_prefix})\"\n    else:\n        summary_info[\"id\"] = f\"{dsid_common_prefix}\"\n    qc_summary[\"info\"] = summary_info\n\n    # Save JSON file\n    timestamp = _timestamp_filename\n    fileid = hashlib.md5(_timestamp_with_ms.encode()).hexdigest()\n    infostr = re.sub(\"[^a-z0-9]\", \"\", info.lower())[:10] if info else \"\"\n    filename = f\"qa_result_{infostr}{'_' if infostr else ''}{timestamp}_{fileid}.json\"\n    with open(os.path.join(result_dir, filename), \"w\") as f:\n        json.dump(qc_summary, f, indent=4, ensure_ascii=False, sort_keys=False)\n    print(f\"Saved QC result: {result_dir}/{filename}\")\n\n    # Save cluster\n    summary.cluster_summary()\n    qc_summary_clustered = summary.clustered_summary\n    # print(json.dumps(qc_summary_clustered, indent=4))\n    qc_summary_clustered[\"info\"] = summary_info\n    filename = (\n        f\"qa_result_{infostr}{'_' if infostr else ''}{timestamp}_{fileid}.cluster.json\"\n    )\n    with open(os.path.join(result_dir, filename), \"w\") as f:\n        json.dump(\n            qc_summary_clustered, f, indent=4, ensure_ascii=False, sort_keys=False\n        )\n    print(f\"Saved QC cluster summary: {result_dir}/{filename}\")\n</code></pre>"},{"location":"reference/esgf_qa_run_qa/#esgf_qa.run_qa.parse_options","title":"<code>parse_options(opts)</code>","text":"<p>Helper function to parse possible options. Splits option into key/value pairs and optionally a value for the checker option. The separator is a colon. Adapted from https://github.com/ioos/compliance-checker/blob/cbb40ed1981c169b74c954f0775d5bd23005ed23/cchecker.py#L23</p> <p>Parameters:</p> Name Type Description Default <code>opts</code> <code>Iterable of strings</code> <p>Iterable of option strings</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with keys as checker type (i.e. \"mip\"). Each value is a dictionary where keys are checker options and values are checker option values or None if not provided.</p> Source code in <code>esgf_qa/run_qa.py</code> <pre><code>def parse_options(opts):\n    \"\"\"\n    Helper function to parse possible options. Splits option into key/value\n    pairs and optionally a value for the checker option. The separator\n    is a colon. Adapted from\n    https://github.com/ioos/compliance-checker/blob/cbb40ed1981c169b74c954f0775d5bd23005ed23/cchecker.py#L23\n\n    Parameters\n    ----------\n    opts : Iterable of strings\n        Iterable of option strings\n\n    Returns\n    -------\n    dict\n        Dictionary with keys as checker type (i.e. \"mip\").\n        Each value is a dictionary where keys are checker options and values\n        are checker option values or None if not provided.\n    \"\"\"\n    options_dict = defaultdict(dict)\n    for opt_str in opts:\n        try:\n            checker_type, checker_opt, *checker_val = opt_str.split(\":\", 2)\n            checker_val = checker_val[0] if checker_val else None\n        except ValueError:\n            raise ValueError(\n                f\"Could not split option '{opt_str}', seems illegally formatted. The required format is: '&lt;checker&gt;:&lt;option_name&gt;[:&lt;option_value&gt;]', eg. 'mip:tables:/path/to/Tables'.\"\n            )\n        if checker_type != \"mip\":\n            raise ValueError(\n                f\"Currently, only options for 'mip' checker are supported, got '{checker_type}'.\"\n            )\n        options_dict[checker_type][checker_opt] = checker_val\n    return options_dict\n</code></pre>"},{"location":"reference/esgf_qa_run_qa/#esgf_qa.run_qa.process_dataset","title":"<code>process_dataset(ds, ds_map, checkers, checker_options, files_to_check_dict, processed_datasets, progress_file)</code>","text":"<p>Runs esgf_qa checks on a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>str</code> <p>Dataset to process.</p> required <code>ds_map</code> <code>dict</code> <p>Dictionary mapping dataset IDs to file paths.</p> required <code>checkers</code> <code>list</code> <p>List of checkers to run.</p> required <code>checker_options</code> <code>dict</code> <p>Dictionary of checker options.</p> required <code>files_to_check_dict</code> <code>dict</code> <p>A special dictionary mapping files to check to datasets.</p> required <code>processed_datasets</code> <code>set</code> <p>Set of processed datasets.</p> required <code>progress_file</code> <code>str</code> <p>Path to progress file.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>Dataset ID and check results.</p> Source code in <code>esgf_qa/run_qa.py</code> <pre><code>def process_dataset(\n    ds,\n    ds_map,\n    checkers,\n    checker_options,\n    files_to_check_dict,\n    processed_datasets,\n    progress_file,\n):\n    \"\"\"\n    Runs esgf_qa checks on a dataset.\n\n    Parameters\n    ----------\n    ds : str\n        Dataset to process.\n    ds_map : dict\n        Dictionary mapping dataset IDs to file paths.\n    checkers : list\n        List of checkers to run.\n    checker_options : dict\n        Dictionary of checker options.\n    files_to_check_dict : dict\n        A special dictionary mapping files to check to datasets.\n    processed_datasets : set\n        Set of processed datasets.\n    progress_file : str\n        Path to progress file.\n\n    Returns\n    -------\n    tuple\n        Dataset ID and check results.\n    \"\"\"\n    # Read result from disk if check was run previously\n    result_file = files_to_check_dict[ds_map[ds][0]][\"result_file_ds\"]\n    if ds in processed_datasets and os.path.isfile(result_file):\n        with open(result_file) as file:\n            print(f\"Read result from disk for '{ds}'.\")\n            result = json.load(file)\n        # If no runtime errors were registered last time, return results, otherwise rerun checks\n        # Potentially add more conditions to rerun checks:\n        #  eg. rerun checks if runtime errors occured\n        #      rerun checks if lvl 1 checks failed\n        #      rerun checks if lvl 1 and 2 checks failed\n        #      rerun checks if any checks failed\n        #      rerun checks if forced by user\n        if all(\n            result[checker.split(\":\")[0]][\"errors\"] == {}\n            for checker in checkers\n            if checker.split(\":\")[0] in result\n            and \"errors\" in result[checker.split(\":\")[0]]\n        ):\n            return ds, result\n        else:\n            print(f\"Rerunning previously erroneous checks for '{ds}'.\")\n    else:\n        print(f\"Running checks for '{ds}'.\")\n\n    # Else run check\n    result = dict()\n    for checkerv in checkers:\n        checker = checkerv.split(\":\")[0]\n        if checker in globals():\n            checker_fct = globals()[checker]\n            result[checker] = checker_fct(\n                ds, ds_map, files_to_check_dict, checker_options[checker]\n            )\n        else:\n            result[checker] = {\n                \"errors\": {\n                    checker: {\n                        \"msg\": f\"Checker '{checker}' not found.\",\n                        \"files\": ds_map[ds],\n                    },\n                },\n            }\n\n    # Write result to disk\n    with open(result_file, \"w\") as f:\n        json.dump(result, f, ensure_ascii=False, indent=4)\n\n    # Register file in progress file\n    with open(progress_file, \"a\") as file:\n        file.write(ds + \"\\n\")\n\n    return ds, result\n</code></pre>"},{"location":"reference/esgf_qa_run_qa/#esgf_qa.run_qa.process_file","title":"<code>process_file(file_path, checkers, checker_options, files_to_check_dict, processed_files, progress_file)</code>","text":"<p>Runs cc checks for a single file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the file to be checked.</p> required <code>checkers</code> <code>list</code> <p>A list of checkers to run.</p> required <code>checker_options</code> <code>dict</code> <p>A dictionary of options for the checkers.</p> required <code>files_to_check_dict</code> <code>dict</code> <p>A special dictionary mapping files to check to datasets.</p> required <code>processed_files</code> <code>list</code> <p>A list of files that have already been checked.</p> required <code>progress_file</code> <code>str</code> <p>The path to the progress file.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the file path and the results of the compliance checker.</p> Source code in <code>esgf_qa/run_qa.py</code> <pre><code>def process_file(\n    file_path,\n    checkers,\n    checker_options,\n    files_to_check_dict,\n    processed_files,\n    progress_file,\n):\n    \"\"\"\n    Runs cc checks for a single file.\n\n    Parameters\n    ----------\n    file_path : str\n        The path to the file to be checked.\n    checkers : list\n        A list of checkers to run.\n    checker_options : dict\n        A dictionary of options for the checkers.\n    files_to_check_dict : dict\n        A special dictionary mapping files to check to datasets.\n    processed_files : list\n        A list of files that have already been checked.\n    progress_file : str\n        The path to the progress file.\n\n    Returns\n    -------\n    tuple\n        A tuple containing the file path and the results of the compliance checker.\n    \"\"\"\n    # Read result from disk if check was run previously\n    result_file = files_to_check_dict[file_path][\"result_file\"]\n    consistency_file = files_to_check_dict[file_path][\"consistency_file\"]\n    if (\n        file_path in processed_files\n        and os.path.isfile(result_file)\n        and (\n            os.path.isfile(consistency_file)\n            or not any(cn.startswith(\"cc6\") or cn.startswith(\"mip\") for cn in checkers)\n        )\n    ):\n        with open(result_file) as file:\n            print(f\"Read result from disk for '{file_path}'.\")\n            result = json.load(file)\n        # If no runtime errors were registered last time, return results, otherwise rerun checks\n        # Potentially add more conditions to rerun checks:\n        #  eg. rerun checks if runtime errors occured\n        #      rerun checks if lvl 1 checks failed\n        #      rerun checks if lvl 1 and 2 checks failed\n        #      rerun checks if any checks failed\n        #      rerun checks if forced by user\n        if all(result[checker.split(\":\")[0]][\"errors\"] == {} for checker in checkers):\n            return file_path, result\n        else:\n            print(f\"Rerunning previously erroneous checks for '{file_path}'.\")\n    else:\n        print(f\"Running checks for '{file_path}'.\")\n\n    # Else run check\n    result = run_compliance_checker(file_path, checkers, checker_options)\n\n    # Check result\n    check_results = dict()\n    # Note: the key in the errors dict is not the same as the check name!\n    #       The key is the checker function name, while the check.name\n    #       is the description.\n    for checkerv in checkers:\n        checker = checkerv.split(\":\")[0]\n        check_results[checker] = dict()\n        check_results[checker][\"errors\"] = {}\n        # print()\n        # print(\"name\",result[checker][0][0].name)\n        # print(\"weight\", result[checker][0][0].weight)\n        # print(\"value\", result[checker][0][0].value)\n        # print(\"msgs\", result[checker][0][0].msgs)\n        # print(\"method\", result[checker][0][0].check_method)\n        # print(\"children\", result[checker][0][0].children)\n        # quit()\n        for check in result[checkerv][0]:\n            check_results[checker][check.name] = {}\n            check_results[checker][check.name][\"weight\"] = check.weight\n            check_results[checker][check.name][\"value\"] = check.value\n            check_results[checker][check.name][\"msgs\"] = check.msgs\n            check_results[checker][check.name][\"method\"] = check.check_method\n            check_results[checker][check.name][\"children\"] = check.children\n        for check_method in result[checkerv][1]:\n            a = result[checkerv][1][check_method][1]\n            while True:\n                if a.tb_frame.f_code.co_name == check_method:\n                    break\n                else:\n                    a = a.tb_next\n            check_results[checker][\"errors\"][\n                check_method\n            ] = f\"Exception: {result[checkerv][1][check_method][0]} at {a.tb_frame.f_code.co_filename}:{a.tb_frame.f_lineno} in function/method '{a.tb_frame.f_code.co_name}'.\"\n            vars = [\n                j\n                for i, j in a.tb_frame.f_locals.items()\n                if \"var\" in i and isinstance(j, str)\n            ]\n            if vars:\n                check_results[checker][\"errors\"][\n                    check_method\n                ] += f\" Potentially affected variables: {', '.join(vars)}.\"\n\n    # Write result to disk\n    with open(result_file, \"w\") as f:\n        json.dump(check_results, f, ensure_ascii=False, indent=4)\n\n    # Register file in progress file\n    with open(progress_file, \"a\") as file:\n        file.write(file_path + \"\\n\")\n\n    return file_path, check_results\n</code></pre>"},{"location":"reference/esgf_qa_run_qa/#esgf_qa.run_qa.run_compliance_checker","title":"<code>run_compliance_checker(file_path, checkers, checker_options={})</code>","text":"<p>Run the compliance checker on a file with the specified checkers and options.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the file to be checked.</p> required <code>checkers</code> <code>list</code> <p>A list of checkers to run.</p> required <code>checker_options</code> <code>dict</code> <p>A dictionary of options for the checkers. Example format: {\"cf\": {\"check_dimension_order\": True}}</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the results of the compliance checker.</p> Source code in <code>esgf_qa/run_qa.py</code> <pre><code>def run_compliance_checker(file_path, checkers, checker_options={}):\n    \"\"\"\n    Run the compliance checker on a file with the specified checkers and options.\n\n    Parameters\n    ----------\n    file_path : str\n        The path to the file to be checked.\n    checkers : list\n        A list of checkers to run.\n    checker_options : dict, optional\n        A dictionary of options for the checkers.\n        Example format: {\"cf\": {\"check_dimension_order\": True}}\n\n    Returns\n    -------\n    dict\n        A dictionary containing the results of the compliance checker.\n    \"\"\"\n    check_suite = CheckSuite(options=checker_options)\n    check_suite.load_all_available_checkers()\n    ds = check_suite.load_dataset(file_path)\n    include_checks = None\n    # Run only time checks if time_checks_only option is set\n    if checker_options.get(\"cc6\", {}).get(\n        \"time_checks_only\", False\n    ) or checker_options.get(\"mip\", {}).get(\"time_checks_only\", False):\n        include_checks = [\n            \"check_time_continuity\",\n            \"check_time_bounds\",\n            \"check_time_range\",\n        ]\n    else:\n        include_checks = None\n    if include_checks:\n        results = {}\n        for checker in checkers:\n            if include_checks and \"cc6:latest\" in checker or \"mip:latest\" in checker:\n                results.update(\n                    check_suite.run_all(ds, [checker], include_checks, skip_checks=[])\n                )\n            else:\n                results.update(\n                    check_suite.run_all(\n                        ds, [checker], include_checks=None, skip_checks=[]\n                    )\n                )\n        return results\n    return check_suite.run_all(ds, checkers, include_checks=None, skip_checks=[])\n</code></pre>"},{"location":"reference/esgf_qa_run_qa/#esgf_qa.run_qa.track_checked_datasets","title":"<code>track_checked_datasets(checked_datasets_file, checked_datasets)</code>","text":"<p>Track checked datasets.</p> <p>Parameters:</p> Name Type Description Default <code>checked_datasets_file</code> <code>str</code> <p>The path to the file to track checked datasets.</p> required <code>checked_datasets</code> <code>list</code> <p>A list of checked datasets.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Writes the checked datasets to the file.</p> Source code in <code>esgf_qa/run_qa.py</code> <pre><code>def track_checked_datasets(checked_datasets_file, checked_datasets):\n    \"\"\"\n    Track checked datasets.\n\n    Parameters\n    ----------\n    checked_datasets_file : str\n        The path to the file to track checked datasets.\n    checked_datasets : list\n        A list of checked datasets.\n\n    Returns\n    -------\n    None\n        Writes the checked datasets to the file.\n    \"\"\"\n    with open(checked_datasets_file, \"a\") as file:\n        writer = csv.writer(file)\n        for dataset_id in checked_datasets:\n            writer.writerow([dataset_id])\n</code></pre>"}]}